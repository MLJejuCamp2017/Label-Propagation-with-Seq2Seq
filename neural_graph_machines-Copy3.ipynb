{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "UNK_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "with open('./nmt_data/vocab.en', 'r') as f, open('./nmt_data/vocab.vi', 'r') as g:\n",
    "    src_vocab = [x[:-1] for x in f.readlines()]\n",
    "    tgt_vocab = [x[:-1] for x in g.readlines()]\n",
    "    \n",
    "def list_to_dict(vocab_list):\n",
    "    ret = {}\n",
    "    for i in range(len(vocab_list)):\n",
    "        ret[i] = vocab_list[i]\n",
    "    \n",
    "    return ret\n",
    "\n",
    "src_vocab, tgt_vocab = list_to_dict(src_vocab), list_to_dict(tgt_vocab)\n",
    "src_vocab_inv, tgt_vocab_inv = {v: k for k, v in src_vocab.items()}, {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "\n",
    "def word2idx(vocab_inv, word):\n",
    "    try:\n",
    "        ret = vocab_inv[word]\n",
    "    except:\n",
    "        ret = UNK_token\n",
    "    return ret\n",
    "\n",
    "def idx2word(vocab, idx):\n",
    "    return vocab[idx]\n",
    "\n",
    "with open('./nmt_data/train.en', 'r') as f:\n",
    "    ss_L = [[word2idx(src_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "    \n",
    "with open('./nmt_data/mono.en', 'r') as f:\n",
    "    ss_U = [[word2idx(src_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "    \n",
    "l = len(ss_L) #last index of labeled samples\n",
    "u = l + len(ss_U) #last index of all samples\n",
    "\n",
    "sources = ss_L\n",
    "sources0 = np.array(ss_L)\n",
    "sources.extend(ss_U)\n",
    "sources = np.array(sources)\n",
    "\n",
    "with open('./nmt_data/train.vi', 'r') as f:\n",
    "    targets = [[word2idx(tgt_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "\n",
    "targets = np.array(targets)\n",
    "\n",
    "def label(i):\n",
    "    if 0 <= i < l:\n",
    "        return targets[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from embeddings_graph import EmbeddingsGraph\n",
    "\n",
    "batch_size = 128\n",
    "max_time = 50\n",
    "time_major = True\n",
    "\n",
    "graph = EmbeddingsGraph().graph\n",
    "\n",
    "class Dummy(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "def batch_iter(batch_size, sources, targets, ending=False):\n",
    "    \"\"\"\n",
    "        Generates a batch iterator for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    data_size = len(sources)\n",
    "\n",
    "    rand_inds = np.random.permutation(np.arange(data_size))\n",
    "\n",
    "    num_batches = int(data_size / batch_size)\n",
    "\n",
    "    if data_size % batch_size > 0:\n",
    "        num_batches = int(data_size / batch_size) + 1\n",
    "\n",
    "    batch_num = 0\n",
    "    end_flag = False\n",
    "    while True:\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = (batch_num + 1) * batch_size\n",
    "        \n",
    "        if end_index > data_size:\n",
    "            if ending:\n",
    "                end_flag = True\n",
    "            else: \n",
    "                print('rebatching...')\n",
    "                batch_num = 0\n",
    "                rand_inds = np.random.permutation(rand_inds)\n",
    "                start_index = 0\n",
    "                end_index = batch_size\n",
    "        \n",
    "        \n",
    "        srcs = sources[rand_inds[start_index:end_index]]\n",
    "        tgts = targets[rand_inds[start_index:end_index]]\n",
    "        source_sequence_lengths = np.array([np.min([len(x)+1, max_time]) for x in srcs])\n",
    "        target_sequence_lengths = np.array([np.min([len(x)+1, max_time]) for x in tgts])\n",
    "        \n",
    "        srcs = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in srcs])\n",
    "        tgts = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in tgts])\n",
    "        \n",
    "        srcs = srcs.T\n",
    "        tgts = tgts.T\n",
    "        \n",
    "        params = Dummy()\n",
    "        params.source_sequence_lengths = source_sequence_lengths\n",
    "        params.target_sequence_lengths = target_sequence_lengths\n",
    "        params.sources = srcs\n",
    "        params.targets = tgts\n",
    "        \n",
    "        yield params\n",
    "        \n",
    "        if end_flag:\n",
    "            return\n",
    "        \n",
    "        batch_num += 1\n",
    "        \n",
    "def next_batch(h_edges, start, finish):\n",
    "    \"\"\"\n",
    "    Helper function for the iterator, note that the neural graph machines,\n",
    "    due to its unique loss function, requires carefully crafted inputs\n",
    "\n",
    "    Refer to the Neural Graph Machines paper, section 3 and 3.3 for more details\n",
    "    \"\"\"\n",
    "    edges_ll = list()\n",
    "    edges_lu = list()\n",
    "    edges_uu = list()\n",
    "    weights_ll = list()\n",
    "    weights_lu = list()\n",
    "    weights_uu = list()\n",
    "    batch_edges = h_edges[start:finish]\n",
    "    batch_edges = np.asarray(batch_edges)\n",
    "\n",
    "    for i, j in batch_edges[:]:\n",
    "        if (0 <= i < l) and (0 <= j < l):\n",
    "            edges_ll.append((i, j))\n",
    "            weights_ll.append(graph.get_edge_data(i,j)['weight'])\n",
    "        elif (0 <= i < l) and (l <= j < u):\n",
    "            edges_lu.append((i, j))\n",
    "            weights_lu.append(graph.get_edge_data(i,j)['weight'])\n",
    "        else:\n",
    "            edges_uu.append((i, j))\n",
    "            weights_uu.append(graph.get_edge_data(i,j)['weight'])\n",
    "    \n",
    "    if len(edges_ll)==0 or len(edges_lu)==0 or len(edges_uu)==0:\n",
    "        print(\"No matched data. Reset the batch\")\n",
    "        np.random.shuffle(h_edges[start:])\n",
    "        return next_batch(h_edges,start,finish)\n",
    "        \n",
    "\n",
    "    u_ll = [e[0] for e in edges_ll]\n",
    "\n",
    "    # number of incident edges for nodes u\n",
    "    c_ull = [1 / len(graph.edges(n)) for n in u_ll]\n",
    "    v_ll = [e[1] for e in edges_ll]\n",
    "    c_vll = [1 / len(graph.edges(n)) for n in v_ll]\n",
    "    nodes_ll_u = sources[u_ll]\n",
    "    \n",
    "    labels_ll_u = np.empty(len(u_ll), dtype=np.object)\n",
    "    labels_ll_u[:] = [label(n) for n in u_ll]\n",
    "    \n",
    "    nodes_ll_v = sources[v_ll]\n",
    "\n",
    "    labels_ll_v = np.empty(len(v_ll), dtype=np.object)\n",
    "    labels_ll_v[:] = [label(n) for n in v_ll]\n",
    "    \n",
    "    u_lu = [e[0] for e in edges_lu]\n",
    "    c_ulu = [1 / len(graph.edges(n)) for n in u_lu]\n",
    "    nodes_lu_u = sources[u_lu]\n",
    "    nodes_lu_v = sources[[e[1] for e in edges_lu]]\n",
    "\n",
    "    labels_lu = np.empty(len(u_lu), dtype=np.object)\n",
    "    labels_lu[:] = [label(n) for n in u_lu]\n",
    "    \n",
    "    nodes_uu_u = sources[[e[0] for e in edges_uu]]\n",
    "    nodes_uu_v = sources[[e[1] for e in edges_uu]]\n",
    "    \n",
    "    len_in_u1 = [np.min([len(x)+1, max_time]) for x in nodes_ll_u]\n",
    "    len_in_v1 = [np.min([len(x)+1, max_time]) for x in nodes_ll_v]\n",
    "    len_in_u2 = [np.min([len(x)+1, max_time]) for x in nodes_lu_u]\n",
    "    len_in_v2 = [np.min([len(x)+1, max_time]) for x in nodes_lu_v]\n",
    "    len_in_u3 = [np.min([len(x)+1, max_time]) for x in nodes_uu_u]\n",
    "    len_in_v3 = [np.min([len(x)+1, max_time]) for x in nodes_uu_v]\n",
    "    len_out_u1 = [np.min([len(x)+1, max_time]) for x in labels_ll_u]\n",
    "    len_out_v1 = [np.min([len(x)+1, max_time]) for x in labels_ll_v]\n",
    "    len_out_u2 = [np.min([len(x)+1, max_time]) for x in labels_lu]\n",
    "    \n",
    "    nodes_ll_u = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in nodes_ll_u]).T\n",
    "    nodes_ll_v = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in nodes_ll_v]).T\n",
    "    nodes_lu_u = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in nodes_lu_u]).T\n",
    "    nodes_lu_v = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in nodes_lu_v]).T\n",
    "    nodes_uu_u = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in nodes_uu_u]).T\n",
    "    nodes_uu_v = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in nodes_uu_v]).T\n",
    "    labels_ll_u = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in labels_ll_u]).T\n",
    "    labels_ll_v = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in labels_ll_v]).T\n",
    "    labels_lu = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in labels_lu]).T\n",
    "        \n",
    "    params = Dummy()\n",
    "    params.in_u1 = nodes_ll_u\n",
    "    params.in_v1 = nodes_ll_v\n",
    "    params.out_u1 = labels_ll_u\n",
    "    params.out_v1 = labels_ll_v\n",
    "    params.in_u3 = nodes_uu_u\n",
    "    params.in_v3 = nodes_uu_v\n",
    "    params.in_u2 = nodes_lu_u\n",
    "    params.in_v2 = nodes_lu_v\n",
    "    params.out_u2 = labels_lu\n",
    "    params.weights_ll = weights_ll\n",
    "    params.weights_lu = weights_lu\n",
    "    params.weights_uu = weights_uu\n",
    "    params.cu1 = c_ull\n",
    "    params.cv1 = c_vll\n",
    "    params.cu2 = c_ulu\n",
    "        \n",
    "    params.len_in_u1 = len_in_u1\n",
    "    params.len_in_v1 = len_in_v1\n",
    "    params.len_in_u2 = len_in_u2\n",
    "    params.len_in_v2 = len_in_v2\n",
    "    params.len_in_u3 = len_in_u3\n",
    "    params.len_in_v3 = len_in_v3\n",
    "    params.len_out_u1 = len_out_u1\n",
    "    params.len_out_v1 = len_out_v1\n",
    "    params.len_out_u2 = len_out_u2\n",
    "        \n",
    "    return params\n",
    "\n",
    "\n",
    "def batch_iter_ngm(batch_size):\n",
    "    \"\"\"\n",
    "        Generates a batch iterator for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    data_size = len(graph.edges())\n",
    "\n",
    "    edges = np.random.permutation(graph.edges())\n",
    "\n",
    "    num_batches = int(data_size / batch_size)\n",
    "\n",
    "    if data_size % batch_size > 0:\n",
    "        num_batches = int(data_size / batch_size) + 1\n",
    "\n",
    "    batch_num = 0\n",
    "    while True:\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = (batch_num + 1) * batch_size\n",
    "        \n",
    "        if end_index > data_size:\n",
    "            print(\"rebatching...\")\n",
    "            batch_num = 0\n",
    "            edges = np.random.permutation(graph.edges())\n",
    "            start_index = 0\n",
    "            end_index = batch_size\n",
    "            \n",
    "        yield next_batch(edges,start_index,end_index)\n",
    "        batch_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core\n",
    "from tqdm import tqdm\n",
    "\n",
    "# # Building the models\n",
    "\n",
    "# ## The Embedding\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "embedding_size = 256\n",
    "num_units = embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def initialization():\n",
    "    model = Dummy()\n",
    "    model.encoder_inputs = tf.placeholder('int32', [max_time, None], name='encoder_inputs')\n",
    "    model.targets = tf.placeholder('int32', [max_time, None], name='target')\n",
    "    model.decoder_inputs = tf.concat([tf.fill([1, tf.shape(model.targets)[1]], SOS_token), model.targets[:-1,:]], 0)\n",
    "    \n",
    "    model.source_sequence_lengths = tf.placeholder('int32', [None], name='source_sequence_lengths')\n",
    "    model.target_sequence_lengths = tf.placeholder('int32', [None], name='target_sequence_lengths')\n",
    "    \n",
    "    model.dropout = tf.placeholder('float32', [], name='dropout')\n",
    "    model.learning_rate = tf.placeholder('float32', [], name='learning_rate')\n",
    "    model.max_gradient_norm = tf.placeholder('float32', [], name='max_gradient_norm') # often set to a value like 5 or 1\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "def embedding(model):\n",
    "    with tf.variable_scope(\"embedding\", dtype='float32') as scope:\n",
    "        # Embedding\n",
    "        embedding_encoder = tf.get_variable(\"embedding_encoder\", [src_vocab_size, embedding_size])\n",
    "        embedding_decoder = tf.get_variable(\"embedding_decoder\", [tgt_vocab_size, embedding_size])\n",
    "        # Look up embedding:\n",
    "        #   encoder_inputs: [max_time, batch_size]\n",
    "        #   encoder_emp_inp: [max_time, batch_size, embedding_size]\n",
    "        encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder, model.encoder_inputs)\n",
    "        decoder_emb_inp = tf.nn.embedding_lookup(embedding_decoder, model.decoder_inputs)\n",
    "        \n",
    "        model.embedding_encoder = embedding_encoder\n",
    "        model.embedding_decoder = embedding_decoder\n",
    "        model.encoder_emb_inp = encoder_emb_inp\n",
    "        model.decoder_emb_inp = decoder_emb_inp\n",
    "        \n",
    "    return model\n",
    "\n",
    "# ## The Encoder\n",
    "def encoder(model):\n",
    "    with tf.variable_scope(\"encoder\", dtype='float32') as scope:\n",
    "        # Build RNN cell\n",
    "        # Construct forward and backward cells\n",
    "        forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "        forward_cell = tf.contrib.rnn.DropoutWrapper(cell=forward_cell, input_keep_prob=(1.0 - model.dropout))\n",
    "        backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "        backward_cell = tf.contrib.rnn.DropoutWrapper(cell=backward_cell, input_keep_prob=(1.0 - model.dropout))\n",
    "\n",
    "        bi_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            forward_cell, backward_cell, model.encoder_emb_inp, dtype='float32',\n",
    "            sequence_length=model.source_sequence_lengths, time_major=True)\n",
    "        bi_encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "        \n",
    "        encoder_outputs = bi_encoder_outputs\n",
    "        encoder_state = bi_encoder_state\n",
    "            \n",
    "        model.encoder_outputs = encoder_outputs\n",
    "        model.encoder_state = encoder_state\n",
    "        \n",
    "    return model\n",
    "\n",
    "# ## Decoder\n",
    "def decoder(model):\n",
    "    with tf.variable_scope(\"decoder\", dtype='float32') as scope:\n",
    "        \"\"\" Attention Mechanisms \"\"\"\n",
    "        # attention_states: [batch_size, max_time, num_units]\n",
    "        attention_states = tf.transpose(model.encoder_outputs, [1, 0, 2])\n",
    "\n",
    "        # Create an attention mechanism\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "            num_units, attention_states, scale=True,\n",
    "            memory_sequence_length=model.source_sequence_lengths)\n",
    "\n",
    "        # Build RNN cell\n",
    "        cell_list = []\n",
    "        for i in range(2):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell=cell, input_keep_prob=(1.0 - model.dropout))\n",
    "            cell_list.append(cell)\n",
    "        \n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "\n",
    "        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            decoder_cell, attention_mechanism,\n",
    "            attention_layer_size=num_units, name=\"attention\")\n",
    "\n",
    "        decoder_initial_state = decoder_cell.zero_state(tf.shape(model.decoder_emb_inp)[1], 'float32').clone(cell_state=model.encoder_state)\n",
    "        \"\"\"\"\"\"\n",
    "        # Helper\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            model.decoder_emb_inp, model.target_sequence_lengths, time_major=True)\n",
    "        # Decoder\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper, decoder_initial_state)\n",
    "        # Dynamic decoding\n",
    "        outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder,\n",
    "            output_time_major=True,\n",
    "            swap_memory=True,\n",
    "            scope=scope)\n",
    "\n",
    "        #projection\n",
    "        output_layer = layers_core.Dense(tgt_vocab_size, use_bias=False, name=\"output_projection\")\n",
    "        logits = output_layer(outputs.rnn_output)\n",
    "        \n",
    "    model.logits = logits\n",
    "    model.decoder_cell = decoder_cell\n",
    "    model.decoder_initial_state = decoder_initial_state\n",
    "    model.output_layer = output_layer\n",
    "    model.final_context_state = final_context_state\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ## Loss & Gradient computation & optimization\n",
    "\n",
    "def crossent_loss(model):\n",
    "    curr_max_time = tf.shape(model.logits)[0]\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=model.targets[:curr_max_time,:], logits=model.logits)\n",
    "    target_weights = tf.sequence_mask(model.target_sequence_lengths, curr_max_time, dtype=model.logits.dtype)\n",
    "\n",
    "    # When time_major is True\n",
    "    target_weights = tf.transpose(target_weights)\n",
    "\n",
    "    loss = tf.reduce_sum(crossent * target_weights) / tf.to_float(tf.shape(model.decoder_emb_inp)[1])\n",
    "\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def create_model():\n",
    "    model = initialization()\n",
    "    model = embedding(model)\n",
    "    model = encoder(model)\n",
    "    model = decoder(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def distance_loss(model1, model2):\n",
    "    scores_u = tf.concat((model1.final_context_state.cell_state[0].c, model1.final_context_state.cell_state[1].c), 1)\n",
    "    scores_v = tf.concat((model2.final_context_state.cell_state[0].c, model2.final_context_state.cell_state[1].c), 1)\n",
    "    \n",
    "    p = 1.0\n",
    "    epsilon = 1e-6\n",
    "    loss = tf.pow(tf.abs(scores_u - scores_v) + epsilon, p)\n",
    "    loss = tf.pow(tf.reduce_sum(loss, 1), 1/p)\n",
    "    return loss\n",
    "\n",
    "def ngm_optimizer(ngm):\n",
    "\n",
    "    l_vanilla = crossent_loss(ngm.model0)\n",
    "    #l_vanilla = tf.reduce_mean(ngm.cu_ll * vanilla_loss(ngm.model_u1))\n",
    "    #l_vanilla += tf.reduce_mean(ngm.cv_ll * vanilla_loss(ngm.model_v1))\n",
    "    #l_vanilla += tf.reduce_mean(ngm.cu_lu * vanilla_loss(ngm.model_u2))\n",
    "\n",
    "    with tf.variable_scope(tf.get_variable_scope()) as scope:\n",
    "        l_dist1 = ngm.alpha_ll * ngm.weights_ll * distance_loss(ngm.model_u1, ngm.model_v1)\n",
    "        scope.reuse_variables()\n",
    "        l_dist2 = ngm.alpha_lu * ngm.weights_lu * distance_loss(ngm.model_u2, ngm.model_v2)\n",
    "        l_dist3 = ngm.alpha_uu * ngm.weights_uu * distance_loss(ngm.model_u3, ngm.model_v3)\n",
    "    ratio = 1\n",
    "    l_dist = (tf.reduce_mean(l_dist1) + tf.reduce_mean(l_dist2) + tf.reduce_mean(l_dist3)) * ratio\n",
    "    \n",
    "    loss = l_vanilla + l_dist\n",
    "    \n",
    "    # Calculate and clip gradients\n",
    "    parameters = tf.trainable_variables()\n",
    "    gradients = tf.gradients(loss, parameters)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, ngm.model_u1.max_gradient_norm)\n",
    "\n",
    "    # Optimization\n",
    "    optimizer = tf.train.GradientDescentOptimizer(ngm.model_u1.learning_rate)\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, parameters))\n",
    "    \n",
    "    ngm.l_vanilla = l_vanilla\n",
    "    ngm.l_dist = l_dist\n",
    "    ngm.loss = loss\n",
    "    ngm.update_step = update_step\n",
    "    \n",
    "    return ngm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope()) as scope:\n",
    "    ngm = Dummy()\n",
    "    ngm.model0 = create_model()\n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    ngm.model_u1 = create_model()\n",
    "    ngm.model_v1 = create_model()\n",
    "    ngm.model_u2 = create_model()\n",
    "    ngm.model_v2 = create_model()\n",
    "    ngm.model_u3 = create_model()\n",
    "    ngm.model_v3 = create_model()\n",
    "    \n",
    "    ngm.alpha_ll = tf.constant(1., dtype=np.float32, name=\"alpha_ll\")\n",
    "    ngm.alpha_lu = tf.constant(1., dtype=np.float32, name=\"alpha_lu\")\n",
    "    ngm.alpha_uu = tf.constant(.5, dtype=np.float32, name=\"alpha_uu\")\n",
    "\n",
    "    ngm.weights_ll = tf.placeholder(tf.float32, [None], name=\"weights_ll\")\n",
    "    ngm.weights_lu = tf.placeholder(tf.float32, [None], name=\"weights_lu\")\n",
    "    ngm.weights_uu = tf.placeholder(tf.float32, [None], name=\"weights_uu\")\n",
    "\n",
    "    ngm.cu_ll = tf.placeholder(tf.float32, [None], name=\"cu_ll\")\n",
    "    ngm.cv_ll = tf.placeholder(tf.float32, [None], name=\"cv_ll\")\n",
    "    ngm.cu_lu = tf.placeholder(tf.float32, [None], name=\"cu_lu\")      \n",
    "\n",
    "ngm = ngm_optimizer(ngm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ## Running training\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "ls_vanilla, ls_dist = [], []\n",
    "\n",
    "batch0 = batch_iter(batch_size, sources0, targets)\n",
    "batch = batch_iter_ngm(batch_size//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bleu import _bleu_online\n",
    "\n",
    "def arr2stn(vocab, sentences):\n",
    "    def foo_iter(stn):\n",
    "        try:\n",
    "            end_idx = stn.index(EOS_token)\n",
    "        except:\n",
    "            end_idx = len(stn)\n",
    "        return ' '.join([idx2word(vocab, word) for word in stn[:end_idx]])\n",
    "    \n",
    "    sentences = sentences.tolist()\n",
    "    ret = []\n",
    "    \n",
    "    if len(sentences)==0:\n",
    "        stn = sentences\n",
    "        ret.append(foo_iter(stn))\n",
    "        \n",
    "    else:\n",
    "        for stn in sentences:\n",
    "            ret.append(foo_iter(stn))\n",
    "    return ret\n",
    "\n",
    "# # Evaluating the network\n",
    "def evaluation(model):\n",
    "    # In[34]:\n",
    "\n",
    "    model.maximum_iterations = tf.round(tf.reduce_max(model.source_sequence_lengths) * 2)\n",
    "\n",
    "\n",
    "    # In[35]:\n",
    "\n",
    "    with tf.variable_scope('decoder', reuse=True) as scope:\n",
    "    # Dynamic decoding\n",
    "        # Helper\n",
    "        helper_eval = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            model.embedding_decoder, tf.fill([tf.shape(model.decoder_emb_inp)[1]], SOS_token),\n",
    "            EOS_token)\n",
    "        # Decoder\n",
    "        decoder_eval = tf.contrib.seq2seq.BasicDecoder(\n",
    "            model.decoder_cell, helper_eval, model.decoder_initial_state,\n",
    "            output_layer=model.output_layer)\n",
    "\n",
    "        outputs_eval, final_context_state_eval, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder_eval, maximum_iterations=model.maximum_iterations,\n",
    "            swap_memory=True, scope=scope)\n",
    "\n",
    "        model.logits_eval = outputs_eval.rnn_output\n",
    "        \n",
    "    curr_max_time = tf.shape(model.logits)[0]\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=model.targets[:curr_max_time,:], logits=model.logits)\n",
    "    target_weights = tf.sequence_mask(model.target_sequence_lengths, curr_max_time, dtype=model.logits.dtype)\n",
    "\n",
    "    # When time_major is True\n",
    "    target_weights = tf.transpose(target_weights)\n",
    "\n",
    "    loss = tf.reduce_sum(crossent * target_weights) / tf.to_float(tf.shape(model.decoder_emb_inp)[1])\n",
    "    \n",
    "    model.loss_eval = loss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = evaluation(ngm.model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./nmt_data/tst2012.en', 'r') as f:\n",
    "    sources_val = [[word2idx(src_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "sources_val = np.array(sources_val)\n",
    "\n",
    "with open('./nmt_data/tst2012.vi', 'r') as f:\n",
    "    targets_val = [[word2idx(tgt_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "targets_val = np.array(targets_val)\n",
    "\n",
    "res = []\n",
    "\n",
    "with open('./nmt_data/tst2013.en', 'r') as f:\n",
    "    sources_tst = [[word2idx(src_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "sources_tst = np.array(sources_tst)\n",
    "\n",
    "with open('./nmt_data/tst2013.vi', 'r') as f:\n",
    "    targets_tst = [[word2idx(tgt_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "targets_tst = np.array(targets_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 0) 224.732 6.18967\n",
      "200.058\n",
      "No matched data. Reset the batch\n",
      "('1', 100) 186.125 13.8069\n",
      "165.685\n",
      "('1', 200) 165.446 3.76363\n",
      "157.904\n",
      "('1', 300) 156.524 1.67515\n",
      "151.21\n",
      "('1', 400) 158.554 1.180778\n",
      "145.124\n",
      "('1', 500) 163.041 0.949034\n",
      "139.456\n",
      "('1', 600) 142.014 1.187473\n",
      "137.875\n",
      "('1', 700) 167.27 0.9075284\n",
      "138.708\n",
      "('1', 800) 150.604 1.599122\n",
      "129.439\n",
      "('1', 900) 141.81 1.3686364\n",
      "127.809\n",
      "('1', 1000) 128.651 1.15179\n",
      "129.506\n",
      "rebatching...42.688 0.795095\n",
      "('1', 1100) 148.239 0.717521\n",
      "124.38\n",
      "('1', 1200) 134.23 1.3802169\n",
      "121.102\n",
      "('1', 1300) 137.955 1.042663\n",
      "117.182\n",
      "('1', 1400) 132.21 1.2087473\n",
      "115.5\n",
      "('1', 1500) 113.113 2.324384\n",
      "112.746\n",
      "('1', 1600) 126.968 2.331137\n",
      "104.537\n",
      "No matched data. Reset the batch\n",
      "('1', 1700) 109.964 0.945978\n",
      "105.433\n",
      "('1', 1800) 112.725 1.118622\n",
      "100.118\n",
      "No matched data. Reset the batch\n",
      "('1', 1900) 95.3183 1.166516\n",
      "97.0147\n",
      "('1', 2000) 107.25 0.9814677\n",
      "92.4392\n",
      "rebatching...1.1171 2.515686\n",
      "('1', 2100) 102.907 0.450017\n",
      "91.299\n",
      "('1', 2119) 88.9808 0.622243\r"
     ]
    }
   ],
   "source": [
    "t_str = time()\n",
    "display.clear_output(wait=False)\n",
    "f, axarr = plt.subplots(1, 2, figsize=(20,10))\n",
    "for i in range(0, 18000):\n",
    "    params = next(batch)\n",
    "    params0 = next(batch0)\n",
    "    \n",
    "    if i == 0:\n",
    "        learning_rate = 1.\n",
    "        phase = '1'\n",
    "    elif i == 12000:\n",
    "        #saver.restore(sess, './log/ngm_256_l1_%s.ckpt' % phase)\n",
    "        learning_rate = .5\n",
    "        phase = '2'\n",
    "    elif i == 13500:\n",
    "        #saver.restore(sess, './log/ngm_256_l1_%s.ckpt' % phase)\n",
    "        learning_rate = .25\n",
    "        phase = '3'\n",
    "    elif i == 15000:\n",
    "        #saver.restore(sess, './log/ngm_256_l1_%s.ckpt' % phase)\n",
    "        learning_rate = .125\n",
    "        phase = '4'\n",
    "    elif i == 16500:\n",
    "        #saver.restore(sess, './log/ngm_256_l1_%s.ckpt' % phase)\n",
    "        learning_rate = .0625\n",
    "        phase = '5'\n",
    "        \n",
    "    feed_dict={ngm.model0.dropout: .2,\n",
    "               ngm.model0.source_sequence_lengths: params0.source_sequence_lengths,\n",
    "               ngm.model0.target_sequence_lengths: params0.target_sequence_lengths,\n",
    "               ngm.model0.encoder_inputs: params0.sources,\n",
    "               ngm.model0.targets: params0.targets,\n",
    "               \n",
    "               ngm.model_u1.learning_rate: learning_rate,\n",
    "               ngm.model_u1.max_gradient_norm: 5,\n",
    "               ngm.weights_ll: params.weights_ll,\n",
    "               ngm.weights_lu: params.weights_lu,\n",
    "               ngm.weights_uu: params.weights_uu,\n",
    "               ngm.cu_ll: params.cu1,\n",
    "               ngm.cv_ll: params.cv1,\n",
    "               ngm.cu_lu: params.cu2,\n",
    "               \n",
    "               ngm.model_u1.dropout: .2,\n",
    "               ngm.model_v1.dropout: .2,\n",
    "               ngm.model_u2.dropout: .2,\n",
    "               ngm.model_v2.dropout: .2,\n",
    "               ngm.model_u3.dropout: .2,\n",
    "               ngm.model_v3.dropout: .2,\n",
    "               \n",
    "               ngm.model_u1.source_sequence_lengths: params.len_in_u1,\n",
    "               ngm.model_u1.target_sequence_lengths: params.len_out_u1,\n",
    "               ngm.model_u1.encoder_inputs: params.in_u1,\n",
    "               ngm.model_u1.targets: params.out_u1,\n",
    "               \n",
    "               ngm.model_v1.source_sequence_lengths: params.len_in_v1,\n",
    "               ngm.model_v1.target_sequence_lengths: params.len_out_v1,\n",
    "               ngm.model_v1.encoder_inputs: params.in_v1,\n",
    "               ngm.model_v1.targets: params.out_v1,\n",
    "               \n",
    "               ngm.model_u2.source_sequence_lengths: params.len_in_u2,\n",
    "               ngm.model_u2.target_sequence_lengths: params.len_out_u2,\n",
    "               ngm.model_u2.encoder_inputs: params.in_u2,\n",
    "               ngm.model_u2.targets: params.out_u2,\n",
    "               \n",
    "               ngm.model_v2.source_sequence_lengths: params.len_in_v2,\n",
    "               ngm.model_v2.target_sequence_lengths: np.ones(params.in_v2.shape[1], 'float32') * EOS_token,\n",
    "               ngm.model_v2.encoder_inputs: params.in_v2,\n",
    "               ngm.model_v2.targets: np.ones((max_time, params.in_v2.shape[1]), 'int32') * EOS_token,\n",
    "\n",
    "               ngm.model_u3.source_sequence_lengths: params.len_in_u3,\n",
    "               ngm.model_u3.target_sequence_lengths: np.ones(params.in_u3.shape[1], 'float32') * EOS_token,\n",
    "               ngm.model_u3.encoder_inputs: params.in_u3,\n",
    "               ngm.model_u3.targets: np.ones((max_time, params.in_u3.shape[1]), 'int32') * EOS_token,\n",
    "               \n",
    "               ngm.model_v3.source_sequence_lengths: params.len_in_v3,\n",
    "               ngm.model_v3.target_sequence_lengths: np.ones(params.in_v3.shape[1], 'float32') * EOS_token,\n",
    "               ngm.model_v3.encoder_inputs: params.in_v3,\n",
    "               ngm.model_v3.targets: np.ones((max_time, params.in_v3.shape[1]), 'int32') * EOS_token\n",
    "               }\n",
    "\n",
    "\n",
    "    _, l_v, l_d = sess.run([ngm.update_step, ngm.l_vanilla, ngm.l_dist], feed_dict=feed_dict)\n",
    "    ls_vanilla.append(l_v)\n",
    "    ls_dist.append(l_d)\n",
    "    print((phase, i), l_v, l_d, end='\\r')\n",
    "    \n",
    "    if (i % 100 == 0):\n",
    "        batch_val = batch_iter(128, sources_val, targets_val, ending=True)\n",
    "        tmp_vals = []\n",
    "        for params in batch_val:\n",
    "            feed_dict_test={model.dropout: 0.,\n",
    "                            model.source_sequence_lengths: params.source_sequence_lengths,\n",
    "                            model.target_sequence_lengths: params.target_sequence_lengths,\n",
    "                            model.encoder_inputs: params.sources,\n",
    "                            model.targets: params.targets}\n",
    "            tmp_vals.append(sess.run(model.loss_eval, feed_dict_test))\n",
    "        res.append(np.mean(tmp_vals))\n",
    "        if res[-1] == np.min(res):\n",
    "            saver.save(sess, './log/ngm_256_l1_%s.ckpt' % phase)\n",
    "        print()\n",
    "        print(res[-1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "saver.save(sess, './log/ngm_256_l1_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ls_file = open('./ngm_256_l1_ls.txt', 'w')\n",
    "\n",
    "for i in range(len(ls_vanilla)):\n",
    "    ls_file.write(\"%f\\t%f\\n\" % (ls_vanilla[i], ls_dist[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.935811951772326"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_val = batch_iter(128, sources_val, targets_val, ending=True)\n",
    "truths = []\n",
    "preds = []\n",
    "for params in batch_val:\n",
    "    feed_dict_test={model.dropout: 0.,\n",
    "                    model.source_sequence_lengths: params.source_sequence_lengths,\n",
    "                    model.target_sequence_lengths: params.target_sequence_lengths,\n",
    "                    model.encoder_inputs: params.sources,\n",
    "                    model.targets: params.targets}\n",
    "    \n",
    "    truths.extend(arr2stn(tgt_vocab, params.targets.T))\n",
    "    preds.extend(arr2stn(tgt_vocab, np.argmax(sess.run(model.logits_eval, feed_dict_test),2)))\n",
    "    \n",
    "_bleu_online([truths], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.180863850447444"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_val = batch_iter(128, sources_tst, targets_tst, ending=True)\n",
    "truths = []\n",
    "preds = []\n",
    "for params in batch_val:\n",
    "    feed_dict_test={model.dropout: 0.,\n",
    "                    model.source_sequence_lengths: params.source_sequence_lengths,\n",
    "                    model.target_sequence_lengths: params.target_sequence_lengths,\n",
    "                    model.encoder_inputs: params.sources,\n",
    "                    model.targets: params.targets}\n",
    "    \n",
    "    truths.extend(arr2stn(tgt_vocab, params.targets.T))\n",
    "    preds.extend(arr2stn(tgt_vocab, np.argmax(sess.run(model.logits_eval, feed_dict_test),2)))\n",
    "    \n",
    "_bleu_online([truths], preds)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf_r1.0_3.5]",
   "language": "python",
   "name": "conda-env-tf_r1.0_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
