{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## create graph parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "from sklearn.neighbors import LSHForest\n",
    "from multiprocessing import Process, Manager, Value, Lock, cpu_count\n",
    "from time import time\n",
    "import pickle\n",
    "import glob\n",
    "from operator import mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSHForest(min_hash_match=4, n_candidates=50, n_estimators=10, n_neighbors=5,\n",
       "     radius=1.0, radius_cutoff_ratio=0.9, random_state=42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = sp.sparse.load_npz('./data/graph/labeled.npz')\n",
    "U = sp.sparse.load_npz('./data/graph/unlabeled.npz')\n",
    "M = sp.sparse.vstack([L,U])\n",
    "last_index_l = L.shape[0]\n",
    "last_index_u = last_index_l + U.shape[0]\n",
    "\n",
    "# we only keep the closest neighbors\n",
    "max_neighs = 5\n",
    "size = M.shape[0]\n",
    "\n",
    "#lshf = LSHForest(n_estimators=15, n_candidates=50, n_neighbors=6, random_state=42)\n",
    "lshf = LSHForest(random_state=42) \n",
    "lshf.fit(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_load(size,cores):\n",
    "    jobs_per_core = size//cores\n",
    "    job_counts = [jobs_per_core]*cores\n",
    "\n",
    "    split = list(map(mul, job_counts, range(1, cores + 1)))\n",
    "    n = np.sum(job_counts)\n",
    "\n",
    "    if n < size:\n",
    "        split[cores-1] += size-n\n",
    "\n",
    "    ranges = []\n",
    "\n",
    "    for i in range(cores):\n",
    "        if i == 0:\n",
    "            ranges += [range(0,split[i])]\n",
    "        else:\n",
    "            ranges += [range(split[i-1],split[i])]\n",
    "\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_graph_for_embedding(graph,edges_weights,edges_ll,edges_lu,edges_uu,chunk,counter,lock):\n",
    "    end = chunk[-1] + 1\n",
    "    size = len(chunk)\n",
    "    i_str = chunk[0]\n",
    "\n",
    "\n",
    "    batch_size = 1000\n",
    "    batch_num = int(np.ceil(size / batch_size))\n",
    "\n",
    "    sims, inds = [], []\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        t_str = time()\n",
    "        distances, indices = lshf.kneighbors(M[i_str + i*batch_size:int(np.min([i_str + (i+1)*batch_size, end]))],n_neighbors=6)\n",
    "        batch_ids = np.vstack(np.arange(i_str + i*batch_size, int(np.min([i_str + (i+1)*batch_size, end]))))\n",
    "        xs, ys = np.where(indices==batch_ids)\n",
    "        distances[xs,ys] = 2.0\n",
    "        sims.extend(1-distances)\n",
    "        inds.extend(indices)\n",
    "        print(i, time() - t_str, end='\\r')\n",
    "    print()\n",
    "    pickle.dump([sims, inds], open(\"./data/graph/approx_nn.%i.p\" % i_str, \"wb\"))\n",
    "\n",
    "    for c, i in enumerate(chunk):\n",
    "        neighbors_indices = list(inds[c][sims[c].argsort()[-max_neighs::][::-1]])\n",
    "        correct_indices = [np.where(inds[c]==j)[0][0] for j in neighbors_indices if i < j]\n",
    "        graph.update({i:correct_indices})\n",
    "\n",
    "        n = len(correct_indices)\n",
    "\n",
    "        if n > 0:\n",
    "            edges = list(zip([i] * n, correct_indices))\n",
    "            edges_weights.update(dict(zip(edges,np.take(sims[c],correct_indices))))\n",
    "\n",
    "            for j in correct_indices:\n",
    "                if (0 <= i < last_index_l) and (0 <= j < last_index_l):\n",
    "                    edges_ll.append((i,j))\n",
    "                elif (0 <= i < last_index_l) and (last_index_l <= j < last_index_u):\n",
    "                    edges_lu.append((i,j))\n",
    "                else:\n",
    "                    edges_uu.append((i,j))\n",
    "\n",
    "        with lock:\n",
    "            counter.value += 1\n",
    "            print(str(counter.value))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "graph = manager.dict()\n",
    "edges_weights = manager.dict()\n",
    "edges_ll = manager.list()\n",
    "edges_lu = manager.list()\n",
    "edges_uu = manager.list()\n",
    "\n",
    "counter = Value('i', 0)\n",
    "lock = Lock()\n",
    "\n",
    "processes = []\n",
    "num_of_cpu = cpu_count()\n",
    "\n",
    "chunks = split_load(size,num_of_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    p = Process(target=compute_graph_for_embedding,\n",
    "                args=(graph, edges_weights, edges_ll, edges_lu, edges_uu, chunk, counter, lock))\n",
    "    processes += [p]\n",
    "\n",
    "_ = [p.start() for p in processes]\n",
    "_ = [p.join() for p in processes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# save to file the data structure that we worked so hard to compute\n",
    "pickle.dump(dict(graph), open(\"./data/graph/graph.p\", \"wb\"))\n",
    "pickle.dump(dict(edges_weights), open(\"./data/graph/edges_weights.p\", \"wb\"))\n",
    "pickle.dump(list(edges_ll), open(\"./data/graph/edges_ll.p\", \"wb\"))\n",
    "pickle.dump(list(edges_lu), open(\"./data/graph/edges_lu.p\", \"wb\"))\n",
    "pickle.dump(list(edges_uu), open(\"./data/graph/edges_uu.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf_r1.0_3.5]",
   "language": "python",
   "name": "conda-env-tf_r1.0_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
