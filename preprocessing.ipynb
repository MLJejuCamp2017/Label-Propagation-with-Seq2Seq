{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import unicodedata\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_LEN = 10\n",
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(src_path, tgt_path, src_lang='ko', tgt_lang='en'):\n",
    "    \"\"\"\n",
    "    Split parallel dataset into training/validation/test dataset.\n",
    "    Half of train dataset is gonna be monolingual dataset. \n",
    "\n",
    "    Especially, there are some heuristics to clean dataset:\n",
    "        1. character-level separation of sentences.\n",
    "        2. restriction of number of characters.\n",
    "\n",
    "    It'll store splitted dataset under \"data/raw/\" directory. \n",
    "    \"\"\"\n",
    "    # Make directory if it doesn't exist\n",
    "    directory = './data/raw'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Read all parallele dataset into lists\n",
    "    with open(src_path, 'r') as f_src, open(tgt_path, 'r') as f_tgt:\n",
    "        sources_raw = f_src.readlines()\n",
    "        targets_raw = f_tgt.readlines()\n",
    "\n",
    "        ### rule 1. character-level separation of sentences \n",
    "        sources_raw = [unicodedata.normalize(\"NFKD\", unicode_str[:-1]) for unicode_str in sources_raw]\n",
    "        targets_raw = [unicodedata.normalize(\"NFKD\", unicode_str[:-1]) for unicode_str in targets_raw]\n",
    "        ###\n",
    "    \n",
    "    sources_tmp, targets_tmp = [], []\n",
    "    for i in range(len(sources_raw)):\n",
    "        if (MIN_LEN <= len(sources_raw[i]) < MAX_LEN) and (MIN_LEN <= len(targets_raw[i]) < MAX_LEN):\n",
    "            sources_tmp.append(sources_raw[i])\n",
    "            targets_tmp.append(targets_raw[i])\n",
    "    sources_raw, targets_raw = sources_tmp, targets_tmp\n",
    "    \n",
    "    # Convert characters into integers\n",
    "    all_bytes, sources, targets = [], [], []\n",
    "    for i in range(len(sources_raw)):\n",
    "            src = [ord(ch) for ch in sources_raw[i]]\n",
    "            tgt = [ord(ch) for ch in targets_raw[i]]\n",
    "            sources.append(src)\n",
    "            targets.append(tgt)\n",
    "            all_bytes.extend(src+tgt)\n",
    "        \n",
    "    ### rule 2. remove obsolete characters & sentences\n",
    "    unique_bytes = np.unique(all_bytes)\n",
    "\n",
    "    for i in sources:\n",
    "        i.sort()\n",
    "    for i in targets:\n",
    "        i.sort()\n",
    "\n",
    "    banned_char_ids = [0, 61, 96, 97, 98, 101, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 173, 212, 216]\n",
    "    banned_char_ids.extend(list(range(255, len(unique_bytes))))\n",
    "    banned_char_list = unique_bytes[banned_char_ids]\n",
    "    banned_char_list.sort()\n",
    "\n",
    "    def searchsorted(x, y):\n",
    "        idx_x, idx_y = 0, 0\n",
    "        n_x, n_y = len(x), len(y)\n",
    "        while(idx_x<n_x and idx_y<n_y):\n",
    "            if x[idx_x] < y[idx_y]:\n",
    "                idx_x += 1\n",
    "            elif x[idx_x] > y[idx_y]:\n",
    "                idx_y += 1\n",
    "            else:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    banned_ss_ids = []\n",
    "    for i, s in enumerate(sources):\n",
    "        if searchsorted(s, banned_char_list):\n",
    "            banned_ss_ids.append(i)\n",
    "        print(i, end='\\r')\n",
    "\n",
    "    for i in banned_ss_ids[::-1]:\n",
    "        sources_raw.pop(i)\n",
    "        targets_raw.pop(i)\n",
    "    ###\n",
    "\n",
    "    ss_1 = [unicodedata.normalize('NFC', x) for x in sources_raw]\n",
    "    ss_2 = [unicodedata.normalize('NFC', x) for x in targets_raw]\n",
    "\n",
    "    # Shuffle dataset\n",
    "    N = len(ss_1)\n",
    "    ids = np.arange(N)\n",
    "    np.random.shuffle(ids)\n",
    "\n",
    "    with open('%s/%s.total' % (directory, src_lang), 'w') as f_src,\\\n",
    "            open('%s/%s.total' % (directory, tgt_lang), 'w') as f_tgt :\n",
    "        start = 0\n",
    "        end = len(ss_1)\n",
    "        for i in range(start, end):\n",
    "            f_src.write(ss_1[ids[i]]+'\\n')\n",
    "            f_tgt.write(ss_2[ids[i]]+'\\n')\n",
    "\n",
    "    with open('%s/%s.train' % (directory, src_lang), 'w') as f_src,\\\n",
    "            open('%s/%s.train' % (directory, tgt_lang), 'w') as f_tgt:\n",
    "        start = 0\n",
    "        end = start + (N // 10) * 4\n",
    "        for i in range(start, end):\n",
    "            f_src.write(ss_1[ids[i]]+'\\n')\n",
    "            f_tgt.write(ss_2[ids[i]]+'\\n')\n",
    "\n",
    "    with open('%s/%s.train.mono' % (directory, src_lang), 'w') as f_src,\\\n",
    "                open('%s/%s.train.mono' % (directory, tgt_lang), 'w') as f_tgt:\n",
    "            start = (N // 10) * 4\n",
    "            end = start + (N // 10) * 4\n",
    "            for i in range(start, end):\n",
    "                f_src.write(ss_1[ids[i]]+'\\n')\n",
    "                f_tgt.write(ss_2[ids[i]]+'\\n')\n",
    "\n",
    "    with open('%s/%s.valid' % (directory, src_lang), 'w') as f_src,\\\n",
    "                open('%s/%s.valid' % (directory, tgt_lang), 'w') as f_tgt:\n",
    "            start = (N // 10) * 8\n",
    "            end = start + (N // 10) * 1\n",
    "            for i in range(start, end):\n",
    "                f_src.write(ss_1[ids[i]]+'\\n')\n",
    "                f_tgt.write(ss_2[ids[i]]+'\\n')\n",
    "\n",
    "    with open('%s/%s.test' % (directory, src_lang), 'w') as f_src,\\\n",
    "                open('%s/%s.test' % (directory, tgt_lang), 'w') as f_tgt:\n",
    "            start = (N // 10) * 9\n",
    "            end = N\n",
    "            for i in range(start, end):\n",
    "                f_src.write(ss_1[ids[i]]+'\\n')\n",
    "                f_tgt.write(ss_2[ids[i]]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223109\r"
     ]
    }
   ],
   "source": [
    "split_data('./data/raw/crawl_dict_ko.txt', \n",
    "            './data/raw/crawl_dict_en.txt', 'ko', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1215943 1215943\n",
      "149 149\n"
     ]
    }
   ],
   "source": [
    "with open('./data/raw/ko.total', 'r') as f_src, open('./data/raw/en.total', 'r') as f_tgt:\n",
    "    sources_raw = f_src.readlines()\n",
    "    targets_raw = f_tgt.readlines()\n",
    "\n",
    "    sources_raw = [unicodedata.normalize(\"NFKD\", unicode_str[:-1]) for unicode_str in sources_raw]\n",
    "    targets_raw = [unicodedata.normalize(\"NFKD\", unicode_str[:-1]) for unicode_str in targets_raw]\n",
    "    \n",
    "print(len(sources_raw), len(targets_raw))\n",
    "print(np.max([len(x) for x in sources_raw]), np.max([len(x) for x in targets_raw]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_voca(src_lang='ko', tgt_lang='en'):\n",
    "    # Make directory if it doesn't exist\n",
    "    directory = './data/cache'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # load ko-en parallel corpus\n",
    "    with open('./data/raw/%s.total' % src_lang, 'r') as f:\n",
    "        sources_raw = f.readlines()\n",
    "        sources_raw = [unicodedata.normalize(\"NFKD\", unicode_str[:-1]) for unicode_str in sources_raw]\n",
    "\n",
    "    with open('./data/raw/%s.total' % tgt_lang, 'r') as f:\n",
    "        targets_raw = f.readlines()\n",
    "        targets_raw = [unicodedata.normalize(\"NFKD\", unicode_str[:-1]) for unicode_str in targets_raw]\n",
    "\n",
    "    # make character-level parallel corpus\n",
    "    all_bytes, sources, targets = [], [], []\n",
    "    for i in range(len(sources_raw)):\n",
    "        src = [ord(ch) for ch in sources_raw[i]]\n",
    "        tgt = [ord(ch) for ch in targets_raw[i]]\n",
    "        sources.append(src)\n",
    "        targets.append(tgt)\n",
    "        all_bytes.extend(src+tgt)\n",
    "\n",
    "    voca_path = directory + '/preload_voca.pickle'\n",
    "\n",
    "    # make vocabulary\n",
    "    unique_all_bytes = list(np.unique(all_bytes))\n",
    "    unique_all_bytes.sort()\n",
    "    index2byte = [0, 1] + unique_all_bytes  # add <EMP>, <EOS>\n",
    "    byte2index = {}\n",
    "    for i, b in enumerate(index2byte):\n",
    "        byte2index[b] = i\n",
    "    voca_size = len(index2byte)\n",
    "\n",
    "    with open(voca_path, 'wb') as f:\n",
    "        pickle.dump([byte2index, index2byte, voca_size], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_voca('ko', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import  Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_avg_embeddings(word2vec_name='ko_vec', fpath_L='./data/raw/ko.train', fpath_U='./data/raw/ko.train.mono'):\n",
    "\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('./data/utils/%s.bin' % word2vec_name, binary=True)\n",
    "    kkma = Kkma()\n",
    "\n",
    "    # the elements of both matrices below constitute the nodes of our graph\n",
    "    with open(fpath_L, 'r') as f, open(fpath_U, 'r') as g:\n",
    "        ss_L = f.readlines()\n",
    "        ss_L = [x[:-1] for x in ss_L]\n",
    "\n",
    "        ss_U = g.readlines()\n",
    "        ss_U = [x[:-1] for x in ss_U]\n",
    "\n",
    "    # matrix of labeled embeddings\n",
    "    L = np.empty((len(ss_L), model.vector_size), dtype='float32')\n",
    "\n",
    "    # matrix of unlabeled embeddings\n",
    "    U = np.empty((len(ss_U), model.vector_size), dtype='float32')\n",
    "\n",
    "    def word2vec(w):\n",
    "        \"\"\"\n",
    "        with this quick trick I can calculate the embeddings without normalizing the text (removing puctuaction, stop words etc...)\n",
    "        If I pass a word that is not in the word2vec_model, like a stopword or some weird symbol, it just returns a zero vector that\n",
    "        does not cotribute to the avg embedding\n",
    "        \"\"\"\n",
    "        out = np.zeros(model.vector_size)\n",
    "        try:\n",
    "            out = model.word_vec(w)\n",
    "        finally:\n",
    "            return out\n",
    "\n",
    "    i = 0\n",
    "    for s in ss_L:\n",
    "        if word2vec_name=='ko_vec':\n",
    "            words = [x for x, _ in kkma.pos(s)]\n",
    "        else:\n",
    "            words = word_tokenize(s)\n",
    "        \n",
    "        # embedding for review is calculated as average of the embeddings of all words\n",
    "        # this is not ideal but is shown to work reasonably well in literature\n",
    "        # if you need something a bit more sophisticated, look into Doc2Vec algorithms\n",
    "        L[i] = np.mean([word2vec(w) for w in words], axis=0)\n",
    "        print(str(i), end='\\r')\n",
    "        i = i+1\n",
    "    print()\n",
    "    \n",
    "    with open('./data/graph/labeled.pickle', 'wb') as f:\n",
    "        pickle.dump(L, f)\n",
    "\n",
    "\n",
    "    j=0\n",
    "    for s in ss_U:\n",
    "        if word2vec_name=='ko_vec':\n",
    "            words = [x for x, _ in kkma.pos(s)]\n",
    "        else:\n",
    "            words = word_tokenize(s)\n",
    "\n",
    "        U[j] = np.mean([word2vec(w) for w in words], axis=0)\n",
    "        print(str(j), end='\\r')\n",
    "        j = j+1\n",
    "\n",
    "    with open('./data/graph/unlabeled.pickle', 'wb') as f:\n",
    "        pickle.dump(U, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486375\n",
      "486375\r"
     ]
    }
   ],
   "source": [
    "create_avg_embeddings('ko_vec', './data/raw/ko.train', './data/raw/ko.train.mono')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf_r1.0_3.5]",
   "language": "python",
   "name": "conda-env-tf_r1.0_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
