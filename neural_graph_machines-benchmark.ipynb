{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "UNK_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./nmt_data/vocab.en', 'r') as f, open('./nmt_data/vocab.vi', 'r') as g:\n",
    "    src_vocab = [x[:-1] for x in f.readlines()]\n",
    "    tgt_vocab = [x[:-1] for x in g.readlines()]\n",
    "    \n",
    "def list_to_dict(vocab_list):\n",
    "    ret = {}\n",
    "    for i in range(len(vocab_list)):\n",
    "        ret[i] = vocab_list[i]\n",
    "    \n",
    "    return ret\n",
    "\n",
    "src_vocab, tgt_vocab = list_to_dict(src_vocab), list_to_dict(tgt_vocab)\n",
    "src_vocab_inv, tgt_vocab_inv = {v: k for k, v in src_vocab.items()}, {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "\n",
    "def word2idx(vocab_inv, word):\n",
    "    try:\n",
    "        ret = vocab_inv[word]\n",
    "    except:\n",
    "        ret = UNK_token\n",
    "    return ret\n",
    "\n",
    "def idx2word(vocab, idx):\n",
    "    return vocab[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./nmt_data/train.en', 'r') as f:\n",
    "    ss_L = [[word2idx(src_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sources = ss_L\n",
    "sources = np.array(sources)\n",
    "\n",
    "with open('./nmt_data/train.vi', 'r') as f:\n",
    "    targets = [[word2idx(tgt_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_time = 50\n",
    "time_major = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Dummy(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "def batch_iter(batch_size, sources, targets, ending=False):\n",
    "    \"\"\"\n",
    "        Generates a batch iterator for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    data_size = len(sources)\n",
    "\n",
    "    rand_inds = np.random.permutation(np.arange(data_size))\n",
    "\n",
    "    num_batches = int(data_size / batch_size)\n",
    "\n",
    "    if data_size % batch_size > 0:\n",
    "        num_batches = int(data_size / batch_size) + 1\n",
    "\n",
    "    batch_num = 0\n",
    "    end_flag = False\n",
    "    while True:\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = (batch_num + 1) * batch_size\n",
    "        \n",
    "        if end_index > data_size:\n",
    "            if ending:\n",
    "                end_flag = True\n",
    "            else: \n",
    "                print('rebatching...')\n",
    "                batch_num = 0\n",
    "                rand_inds = np.random.permutation(rand_inds)\n",
    "                start_index = 0\n",
    "                end_index = batch_size\n",
    "        \n",
    "        \n",
    "        srcs = sources[rand_inds[start_index:end_index]]\n",
    "        tgts = targets[rand_inds[start_index:end_index]]\n",
    "        source_sequence_lengths = np.array([np.min([len(x)+1, max_time]) for x in srcs])\n",
    "        target_sequence_lengths = np.array([np.min([len(x)+1, max_time]) for x in tgts])\n",
    "        \n",
    "        srcs = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in srcs])\n",
    "        tgts = np.array([x[:max_time-1] + [EOS_token] + [0]*(max_time-len(x)-1) for x in tgts])\n",
    "        \n",
    "        srcs = srcs.T\n",
    "        tgts = tgts.T\n",
    "        \n",
    "        params = Dummy()\n",
    "        params.source_sequence_lengths = source_sequence_lengths\n",
    "        params.target_sequence_lengths = target_sequence_lengths\n",
    "        params.sources = srcs\n",
    "        params.targets = tgts\n",
    "        \n",
    "        yield params\n",
    "        \n",
    "        if end_flag:\n",
    "            return\n",
    "        \n",
    "        batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch = batch_iter(batch_size, sources, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Building the models\n",
    "\n",
    "# ## The Embedding\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "embedding_size = 512\n",
    "num_units = embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def initialization():\n",
    "    model = Dummy()\n",
    "    model.encoder_inputs = tf.placeholder('int32', [max_time, None], name='encoder_inputs')\n",
    "    model.targets = tf.placeholder('int32', [max_time, None], name='target')\n",
    "    model.decoder_inputs = tf.concat([tf.fill([1, tf.shape(model.targets)[1]], SOS_token), model.targets[:-1,:]], 0)\n",
    "    \n",
    "    model.source_sequence_lengths = tf.placeholder('int32', [None], name='source_sequence_lengths')\n",
    "    model.target_sequence_lengths = tf.placeholder('int32', [None], name='target_sequence_lengths')\n",
    "    \n",
    "    model.dropout = tf.placeholder('float32', [], name='dropout')\n",
    "    model.learning_rate = tf.placeholder('float32', [], name='learning_rate')\n",
    "    model.max_gradient_norm = tf.placeholder('float32', [], name='max_gradient_norm') # often set to a value like 5 or 1\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def embedding(model):\n",
    "    with tf.variable_scope(\"embedding\", dtype='float32') as scope:\n",
    "        # Embedding\n",
    "        embedding_encoder = tf.get_variable(\"embedding_encoder\", [src_vocab_size, embedding_size])\n",
    "        embedding_decoder = tf.get_variable(\"embedding_decoder\", [tgt_vocab_size, embedding_size])\n",
    "        # Look up embedding:\n",
    "        #   encoder_inputs: [max_time, batch_size]\n",
    "        #   encoder_emp_inp: [max_time, batch_size, embedding_size]\n",
    "        encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder, model.encoder_inputs)\n",
    "        decoder_emb_inp = tf.nn.embedding_lookup(embedding_decoder, model.decoder_inputs)\n",
    "        \n",
    "        model.embedding_encoder = embedding_encoder\n",
    "        model.embedding_decoder = embedding_decoder\n",
    "        model.encoder_emb_inp = encoder_emb_inp\n",
    "        model.decoder_emb_inp = decoder_emb_inp\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ## The Encoder\n",
    "def encoder(model):\n",
    "    with tf.variable_scope(\"encoder\", dtype='float32') as scope:\n",
    "        # Build RNN cell\n",
    "        # Construct forward and backward cells\n",
    "        forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "        forward_cell = tf.contrib.rnn.DropoutWrapper(cell=forward_cell, input_keep_prob=(1.0 - model.dropout))\n",
    "        backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "        backward_cell = tf.contrib.rnn.DropoutWrapper(cell=backward_cell, input_keep_prob=(1.0 - model.dropout))\n",
    "\n",
    "        bi_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            forward_cell, backward_cell, model.encoder_emb_inp, dtype='float32',\n",
    "            sequence_length=model.source_sequence_lengths, time_major=True)\n",
    "        bi_encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "        \n",
    "        encoder_outputs = bi_encoder_outputs\n",
    "        encoder_state = bi_encoder_state\n",
    "        \"\"\"\n",
    "        # Stacking encoders\n",
    "        encoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "\n",
    "        # Run Dynamic RNN\n",
    "        #   encoder_outpus: [max_time, batch_size, num_units]\n",
    "        #   encoder_state: [batch_size, num_units]\n",
    "        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "            encoder_cell, bi_encoder_outputs, dtype='float32',\n",
    "            sequence_length=model.source_sequence_lengths, time_major=True)\n",
    "        \"\"\"        \n",
    "        model.encoder_outputs = encoder_outputs\n",
    "        model.encoder_state = encoder_state\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ## Decoder\n",
    "def decoder(model):\n",
    "    with tf.variable_scope(\"decoder\", dtype='float32') as scope:\n",
    "        \"\"\" Attention Mechanisms \"\"\"\n",
    "        # attention_states: [batch_size, max_time, num_units]\n",
    "        attention_states = tf.transpose(model.encoder_outputs, [1, 0, 2])\n",
    "\n",
    "        # Create an attention mechanism\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "            num_units, attention_states, scale=True,\n",
    "            memory_sequence_length=model.source_sequence_lengths)\n",
    "\n",
    "        # Build RNN cell\n",
    "        cell_list = []\n",
    "        for i in range(2):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell=cell, input_keep_prob=(1.0 - model.dropout))\n",
    "            cell_list.append(cell)\n",
    "        \n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "\n",
    "        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            decoder_cell, attention_mechanism,\n",
    "            attention_layer_size=num_units, name=\"attention\")\n",
    "\n",
    "        decoder_initial_state = decoder_cell.zero_state(tf.shape(model.decoder_emb_inp)[1], 'float32').clone(cell_state=model.encoder_state)\n",
    "        \"\"\"\"\"\"\n",
    "        # Helper\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            model.decoder_emb_inp, model.target_sequence_lengths, time_major=True)\n",
    "        # Decoder\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper, decoder_initial_state)\n",
    "        # Dynamic decoding\n",
    "        outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder,\n",
    "            output_time_major=True,\n",
    "            swap_memory=True,\n",
    "            scope=scope)\n",
    "\n",
    "        #projection\n",
    "        output_layer = layers_core.Dense(tgt_vocab_size, use_bias=False, name=\"output_projection\")\n",
    "        logits = output_layer(outputs.rnn_output)\n",
    "        \n",
    "    model.logits = logits\n",
    "    model.decoder_cell = decoder_cell\n",
    "    model.decoder_initial_state = decoder_initial_state\n",
    "    model.output_layer = output_layer\n",
    "    model.final_context_state = final_context_state\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ## Loss & Gradient computation & optimization\n",
    "\n",
    "def optimizer(model):\n",
    "    curr_max_time = tf.shape(model.logits)[0]\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=model.targets[:curr_max_time,:], logits=model.logits)\n",
    "    target_weights = tf.sequence_mask(model.target_sequence_lengths, curr_max_time, dtype=model.logits.dtype)\n",
    "\n",
    "    # When time_major is True\n",
    "    target_weights = tf.transpose(target_weights)\n",
    "\n",
    "    loss = tf.reduce_sum(crossent * target_weights) / tf.to_float(tf.shape(model.decoder_emb_inp)[1])\n",
    "\n",
    "    # Calculate and clip gradients\n",
    "    parameters = tf.trainable_variables()\n",
    "    gradients = tf.gradients(loss, parameters)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, model.max_gradient_norm)\n",
    "\n",
    "    # Optimization\n",
    "    optimizer = tf.train.GradientDescentOptimizer(model.learning_rate)\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, parameters))\n",
    "    \n",
    "    model.loss = loss\n",
    "    model.update_step = update_step\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = initialization()\n",
    "    model = embedding(model)\n",
    "    model = encoder(model)\n",
    "    model = decoder(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model = optimizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ## Running training\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#saver = tf.train.Saver()\n",
    "#saver.restore(sess, './log/benchmark_512_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rebatching...\n",
      "rebatching...\n",
      "rebatching...\n",
      "rebatching...\n",
      "rebatching...\n",
      "rebatching...\n",
      "rebatching...\n",
      "5911.572916507721\n"
     ]
    }
   ],
   "source": [
    "t_str = time()\n",
    "for i in range(12000):\n",
    "    \n",
    "    if i==0:\n",
    "        lr = 1.\n",
    "    elif i==8000:\n",
    "        lr = .5\n",
    "    elif i==9000:\n",
    "        lr = .25\n",
    "    elif i==10000:\n",
    "        lr = .125\n",
    "    elif i==11000:\n",
    "        lr = .0625\n",
    "    \n",
    "    params = next(batch)\n",
    "    feed_dict={model.learning_rate: lr,\n",
    "               model.dropout: .2,\n",
    "               model.max_gradient_norm: 5,\n",
    "               model.source_sequence_lengths: params.source_sequence_lengths,\n",
    "               model.target_sequence_lengths: params.target_sequence_lengths,\n",
    "               model.encoder_inputs: params.sources,\n",
    "               model.targets: params.targets}\n",
    "\n",
    "\n",
    "    _, l_val = sess.run([model.update_step, model.loss], feed_dict=feed_dict)\n",
    "    ls.append(l_val)\n",
    "    print(i, end='\\r')\n",
    "print(time() - t_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#saver = tf.train.Saver()\n",
    "#saver.save(sess, './log/benchmark_512_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bleu import _bleu_online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def arr2stn(vocab, sentences):\n",
    "    def foo_iter(stn):\n",
    "        try:\n",
    "            end_idx = stn.index(EOS_token)\n",
    "        except:\n",
    "            end_idx = len(stn)\n",
    "        return ' '.join([idx2word(vocab, word) for word in stn[:end_idx]])\n",
    "    \n",
    "    sentences = sentences.tolist()\n",
    "    ret = []\n",
    "    \n",
    "    if len(sentences)==0:\n",
    "        stn = sentences\n",
    "        ret.append(foo_iter(stn))\n",
    "        \n",
    "    else:\n",
    "        for stn in sentences:\n",
    "            ret.append(foo_iter(stn))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Evaluating the network\n",
    "def evaluation(model):\n",
    "    # In[34]:\n",
    "\n",
    "    model.maximum_iterations = tf.round(tf.reduce_max(model.source_sequence_lengths) * 2)\n",
    "\n",
    "\n",
    "    # In[35]:\n",
    "\n",
    "    with tf.variable_scope('decoder', reuse=True) as scope:\n",
    "    # Dynamic decoding\n",
    "        # Helper\n",
    "        helper_eval = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            model.embedding_decoder, tf.fill([tf.shape(model.decoder_emb_inp)[1]], SOS_token),\n",
    "            EOS_token)\n",
    "        # Decoder\n",
    "        decoder_eval = tf.contrib.seq2seq.BasicDecoder(\n",
    "            model.decoder_cell, helper_eval, model.decoder_initial_state,\n",
    "            output_layer=model.output_layer)\n",
    "\n",
    "        outputs_eval, final_context_state_eval, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder_eval, maximum_iterations=model.maximum_iterations,\n",
    "            swap_memory=True, scope=scope)\n",
    "\n",
    "        model.logits_eval = outputs_eval.rnn_output\n",
    "        \n",
    "    curr_max_time = tf.shape(model.logits)[0]\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=model.targets[:curr_max_time,:], logits=model.logits)\n",
    "    target_weights = tf.sequence_mask(model.target_sequence_lengths, curr_max_time, dtype=model.logits.dtype)\n",
    "\n",
    "    # When time_major is True\n",
    "    target_weights = tf.transpose(target_weights)\n",
    "\n",
    "    loss = tf.reduce_sum(crossent * target_weights) / tf.to_float(tf.shape(model.decoder_emb_inp)[1])\n",
    "    \n",
    "    model.loss_eval = loss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_eval = evaluation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./nmt_data/tst2012.en', 'r') as f:\n",
    "    sources_val = [[word2idx(src_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "sources_val = np.array(sources_val)\n",
    "\n",
    "with open('./nmt_data/tst2012.vi', 'r') as f:\n",
    "    targets_val = [[word2idx(tgt_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "targets_val = np.array(targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.105043353616317"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_val = batch_iter(128, sources_val, targets_val, ending=True)\n",
    "truths = []\n",
    "preds = []\n",
    "for params in batch_val:\n",
    "    feed_dict_test={model.dropout: 0.,\n",
    "                    model.source_sequence_lengths: params.source_sequence_lengths,\n",
    "                    model.target_sequence_lengths: params.target_sequence_lengths,\n",
    "                    model.encoder_inputs: params.sources,\n",
    "                    model.targets: params.targets}\n",
    "    \n",
    "    truths.extend(arr2stn(tgt_vocab, params.targets.T))\n",
    "    preds.extend(arr2stn(tgt_vocab, np.argmax(sess.run(model.logits_eval, feed_dict_test),2)))\n",
    "    \n",
    "_bleu_online([truths], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./nmt_data/tst2013.en', 'r') as f:\n",
    "    sources_tst = [[word2idx(src_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "sources_tst = np.array(sources_tst)\n",
    "\n",
    "with open('./nmt_data/tst2013.vi', 'r') as f:\n",
    "    targets_tst = [[word2idx(tgt_vocab_inv, word)for word in sentence[:-1].split(' ')] for sentence in f.readlines()]\n",
    "targets_tst = np.array(targets_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.340469062282168"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_val = batch_iter(128, sources_tst, targets_tst, ending=True)\n",
    "truths = []\n",
    "preds = []\n",
    "for params in batch_val:\n",
    "    feed_dict_test={model.dropout: 0.,\n",
    "                    model.source_sequence_lengths: params.source_sequence_lengths,\n",
    "                    model.target_sequence_lengths: params.target_sequence_lengths,\n",
    "                    model.encoder_inputs: params.sources,\n",
    "                    model.targets: params.targets}\n",
    "    \n",
    "    truths.extend(arr2stn(tgt_vocab, params.targets.T))\n",
    "    preds.extend(arr2stn(tgt_vocab, np.argmax(sess.run(model.logits_eval, feed_dict_test),2)))\n",
    "    \n",
    "_bleu_online([truths], preds)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
