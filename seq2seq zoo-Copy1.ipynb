{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ByteNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def layer_norm(input, causal=False, name=None):\n",
    "    '''\n",
    "    Layer Normalization\n",
    "    \n",
    "    If the model is causal and using Convnet,\n",
    "    normalize input only according to depth.\n",
    "    '''\n",
    "    with tf.variable_scope('layer_norm', name):\n",
    "        if causal: # Sub Layer Normalization\n",
    "            axis_depth = len(input.get_shape()) - 1\n",
    "            mean, var = tf.nn.moments(input, [axis_depth], keep_dims=True)\n",
    "            out = (input - mean) / tf.sqrt(var)\n",
    "            return out\n",
    "        else: # Layer Normalization\n",
    "            axes = np.arange(len(input.get_shape()) - 1) + 1\n",
    "            mean, var = tf.nn.moments(input, axes, keep_dims=True)\n",
    "            out = (input - mean) / tf.sqrt(var)\n",
    "            return out\n",
    "\n",
    "def convolution(input, filter, padding, strides=None, dilation_rate=None, causal=False, name=None):\n",
    "    '''\n",
    "    Masked Convolution\n",
    "    \n",
    "    See PixelCNN\n",
    "    '''\n",
    "    with tf.variable_scope('masked_convolution', name):\n",
    "        filter_shape = filter.get_shape().as_list()\n",
    "        filter_len = np.prod(filter_shape[:-2])\n",
    "        center = filter_len // 2\n",
    "        if causal:\n",
    "            mask = np.ones([filter_len] + filter_shape[-2:], dtype='float32')\n",
    "            mask[center+1: ,: ,:] = 0.\n",
    "            mask = mask.reshape(filter_shape)\n",
    "            \n",
    "            mask = tf.constant(mask, dtype='float32')\n",
    "            filter = filter * mask\n",
    "\n",
    "\n",
    "        ret = tf.nn.convolution(input, filter, padding=padding, strides=strides,\n",
    "                                dilation_rate=dilation_rate, name=name)\n",
    "        \n",
    "    return ret\n",
    "\n",
    "\n",
    "def res_block(input, filter_size=3, dilation_rate=None, causal=False, name='res_block'):\n",
    "    '''\n",
    "    Residual block\n",
    "    \n",
    "    For details, see Ch3.6(Fig 3. Left) of 'Neural Machine Translation in Linear Time(https://arxiv.org/abs/1610.10099)'.\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        x = input\n",
    "        \n",
    "        # input dimension\n",
    "        in_dim = input.get_shape().as_list()[-1]\n",
    "    \n",
    "        # normalization\n",
    "        x = layer_norm(x, causal)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # reduce dimension\n",
    "        w_shape = [1, in_dim, in_dim//2]\n",
    "        w_stddev = np.sqrt(2./np.prod(w_shape[:-1])) # He's init\n",
    "        w = tf.get_variable(shape=w_shape, initializer=tf.random_normal_initializer(stddev=w_stddev),\n",
    "                            name='w1')\n",
    "        x = tf.nn.convolution(x, w, padding='SAME')\n",
    "        x = layer_norm(x, causal)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # 1xk conv dilated (with mask)\n",
    "        w_shape = [filter_size, in_dim//2, in_dim//2]\n",
    "        if causal:\n",
    "            w_stddev = np.sqrt(2. / (np.prod(w_shape[1:-1]) * (filter_size//2 + 1)))\n",
    "        else:\n",
    "            w_stddev = np.sqrt(2./np.prod(w_shape[:-1])) # He's init\n",
    "        w = tf.get_variable(shape=w_shape, initializer=tf.random_normal_initializer(stddev=w_stddev),\n",
    "                            name='w2')\n",
    "        x = convolution(x, w, padding='SAME', dilation_rate=dilation_rate, causal=causal)\n",
    "        x = layer_norm(x, causal)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # dimension recover and residual connection\n",
    "        w_shape = [1, in_dim//2, in_dim]\n",
    "        w_stddev = np.sqrt(2./np.prod(w_shape[:-1])) # He's init\n",
    "        w = tf.get_variable(shape=w_shape, initializer=tf.random_normal_initializer(stddev=w_stddev),\n",
    "                            name='w3')\n",
    "        x = tf.nn.convolution(x, w, padding='SAME')\n",
    "        \n",
    "        # residual connection\n",
    "        x = x + input\n",
    "        \n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def encoder(input, filter_size=3, num_block_sets=6):\n",
    "    '''\n",
    "    Encoder for Character-Level Machine Translation\n",
    "    \n",
    "    For details, see Ch6 of 'Neural Machine Translation in Linear Time(https://arxiv.org/abs/1610.10099)'.\n",
    "    '''\n",
    "    with tf.variable_scope('encoder'):\n",
    "        x = input\n",
    "        for i in range(num_block_sets):\n",
    "            for j in [1,2,4,8,16]:\n",
    "                x = res_block(x, filter_size=filter_size, dilation_rate=[j], name='res_block_%d_%d' % (i, j))\n",
    "        \n",
    "    return x\n",
    "\n",
    "def decoder(input, filter_size=3, num_block_sets=6):\n",
    "    '''\n",
    "    Decoder for Character-Level Machine Translation\n",
    "    \n",
    "    For details, see Ch6 of 'Neural Machine Translation in Linear Time(https://arxiv.org/abs/1610.10099)'.\n",
    "    '''\n",
    "    with tf.variable_scope('decoder'):\n",
    "        x = input\n",
    "        for i in range(num_block_sets):\n",
    "            for j in [1,2,4,8,16]:\n",
    "                x = res_block(x, filter_size=filter_size, dilation_rate=[j],\n",
    "                              causal=True, name='res_block_%d_%d' % (i, j))\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ByteNet(object):\n",
    "    \"\"\"\n",
    "    ByteNet\n",
    "\n",
    "    For details, see 'Neural Machine Translation in Linear Time(https://arxiv.org/abs/1610.10099)'.\n",
    "    \"\"\" \n",
    "\n",
    "    def __init__(self, input_dim=254, input_max_len=150, latent_dim=200, num_block_sets=4):\n",
    "        self.input_dim = input_dim\n",
    "        self.input_max_len = input_max_len\n",
    "        self.filter_size = 3\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_block_sets = num_block_sets\n",
    "\n",
    "    def encoder(self, x):\n",
    "        #\n",
    "        # inputs\n",
    "        #\n",
    "        with tf.variable_scope('input'):\n",
    "            # make embedding matrix for source and target\n",
    "            emb_x = tf.get_variable(shape=[self.input_dim, self.latent_dim],\n",
    "                                    initializer=tf.random_uniform_initializer(-1.0, 1.0),\n",
    "                                    name='emb_x')\n",
    "\n",
    "        #\n",
    "        # encode graph ( atrous convolution )\n",
    "        #\n",
    "\n",
    "        # embed table lookup\n",
    "        enc_emb = tf.nn.embedding_lookup(emb_x, x)\n",
    "        enc = encoder(enc_emb, filter_size=self.filter_size, num_block_sets=self.num_block_sets)\n",
    "\n",
    "        return enc\n",
    "\n",
    "    def decoder(self, enc, y, p_keep_conv):\n",
    "        #\n",
    "        # inputs\n",
    "        #\n",
    "        with tf.variable_scope('input'):\n",
    "            emb_y = tf.get_variable(shape=[self.input_dim, self.latent_dim], \n",
    "                                    initializer=tf.random_uniform_initializer(-1.0, 1.0),\n",
    "                                    name='emb_y')\n",
    "            y_src = tf.pad(y[:,:-1], [[0,0],[1,0]])\n",
    "\n",
    "        #\n",
    "        # decode graph ( causal convolution )\n",
    "        #\n",
    "\n",
    "        # loop dilated causal conv block\n",
    "        dec_emb = tf.concat([enc, tf.nn.embedding_lookup(emb_y, y_src)], 2)\n",
    "        dec = decoder(dec_emb, filter_size=self.filter_size, num_block_sets=self.num_block_sets)\n",
    "\n",
    "\n",
    "        with tf.variable_scope('output'):\n",
    "            # additional convolution and relu\n",
    "            out = layer_norm(dec, causal=True)\n",
    "            out = tf.nn.relu(out)\n",
    "            out_dim = out.get_shape().as_list()[-1] # latent_dim * 2\n",
    "            w_shape = [1, out_dim, out_dim]\n",
    "            w_stddev = np.sqrt(2./np.prod(w_shape[:-1])) # He's init\n",
    "            w = tf.get_variable(shape=w_shape, initializer=tf.random_normal_initializer(stddev=w_stddev),\n",
    "                                name='w1')\n",
    "            out = tf.nn.convolution(out, w, padding='SAME')\n",
    "\n",
    "            # final fully convolution layer for softmax\n",
    "            logits = layer_norm(out, causal=True)\n",
    "            logits = tf.nn.relu(logits)\n",
    "\n",
    "            logits = tf.nn.dropout(logits, p_keep_conv)\n",
    "\n",
    "            w_shape = [1, out_dim, self.input_dim]\n",
    "            w_stddev = np.sqrt(2./np.prod(w_shape[:-1])) # He's init\n",
    "            w = tf.get_variable(shape=w_shape, initializer=tf.random_normal_initializer(stddev=w_stddev),\n",
    "                                name='w2')\n",
    "            logits = tf.nn.convolution(logits, w, padding='SAME')\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test\n",
    "\n",
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "from preprocess import MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### NGM (Neural Graph Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Train data loaded.(total data=486376, total batch=15199)\n",
      "INFO:tensorflow:Train data loaded.(total data=486376, total batch=15199)\n"
     ]
    }
   ],
   "source": [
    "from preprocess import MAX_LEN\n",
    "from batch import batch_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "latent_dim = 100   # hidden layer dimension\n",
    "num_block_sets = 2     # dilated blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tower_loss1(model, params):\n",
    "    p_keep_conv, alpha1, alpha2, alpha3, in_u1, in_v1, in_u2,\\\n",
    "    in_v2, in_u3, in_v3, labels_u1, labels_v1,\\\n",
    "    labels_u2, weights_ll, weights_lu, weights_uu,\\\n",
    "    cu1, cv1, cu2, labels_zero_1, labels_zero_2, labels_zero_3\\\n",
    "    = params\n",
    "    \n",
    "    with tf.variable_scope(tf.get_variable_scope()) as scope:\n",
    "        enc_u1 = model.encoder(in_u1)\n",
    "        logits_u1 = model.decoder(enc_u1, labels_u1, p_keep_conv)\n",
    "        \n",
    "        scope.reuse_variables()\n",
    "        \n",
    "        enc_v1 = model.encoder(in_v1)\n",
    "        enc_u2 = model.encoder(in_u2)\n",
    "\n",
    "        logits_v1 = model.decoder(enc_v1, labels_v1, p_keep_conv)\n",
    "        logits_u2 = model.decoder(enc_u2, labels_u2, p_keep_conv)\n",
    "        \n",
    "    loss_function = tf.reduce_mean(cu1 * vanilla_loss(logits_u1, labels_u1))\\\n",
    "                    + tf.reduce_mean(cv1 * vanilla_loss(logits_v1, labels_v1))\\\n",
    "                    + tf.reduce_mean(cu2 * vanilla_loss(logits_u2, labels_u2))\n",
    "            \n",
    "    return loss_function, enc_u1, enc_v1, enc_u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tower_loss2(model, params, enc_u1, enc_v1, enc_u2):\n",
    "    p_keep_conv, alpha1, alpha2, alpha3, in_u1, in_v1, in_u2,\\\n",
    "    in_v2, in_u3, in_v3, labels_u1, labels_v1,\\\n",
    "    labels_u2, weights_ll, weights_lu, weights_uu,\\\n",
    "    cu1, cv1, cu2, labels_zero_1, labels_zero_2, labels_zero_3\\\n",
    "    = params\n",
    "    \n",
    "    with tf.variable_scope(tf.get_variable_scope()) as scope:\n",
    "        \n",
    "        scope.reuse_variables()\n",
    "        \n",
    "        enc_v2 = model.encoder(in_v2)\n",
    "        enc_u3 = model.encoder(in_u3)\n",
    "        enc_v3 = model.encoder(in_v3)\n",
    "        \n",
    "        scores_u1 = enc_u1\n",
    "        scores_v1 = enc_v1\n",
    "        scores_u2 = enc_u2\n",
    "        scores_v2 = enc_v2\n",
    "        scores_u3 = enc_u3\n",
    "        scores_v3 = enc_v3\n",
    "        #scores_u1 = model.decoder(enc_u1, labels_zero_1, p_keep_conv)\n",
    "        #scores_v1 = model.decoder(enc_v1, labels_zero_1, p_keep_conv)\n",
    "        #scores_u2 = model.decoder(enc_u2, labels_zero_2, p_keep_conv)\n",
    "        #scores_v2 = model.decoder(enc_v2, labels_zero_2, p_keep_conv)\n",
    "        #scores_u3 = model.decoder(enc_u3, labels_zero_3, p_keep_conv)\n",
    "        #scores_v3 = model.decoder(enc_v3, labels_zero_3, p_keep_conv)\n",
    "    \n",
    "    loss_function = tf.reduce_mean(alpha1 * weights_ll * distance_loss(scores_u1, scores_v1))\\\n",
    "                    + tf.reduce_mean(alpha2 * weights_lu * distance_loss(scores_u2, scores_v2))\\\n",
    "                    + tf.reduce_mean(alpha3 * weights_uu * distance_loss(scores_u3, scores_v3))\n",
    "            \n",
    "    return loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "    Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "      is over individual gradients. The inner list is over the gradient\n",
    "      calculation for each tower.\n",
    "    Returns:\n",
    "     List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "     across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_params():\n",
    "    p_keep_conv = tf.placeholder(tf.float32, [])\n",
    "\n",
    "    alpha1 = tf.constant(0.10, dtype=np.float32, name=\"a1\")\n",
    "    alpha2 = tf.constant(0.10, dtype=np.float32, name=\"a2\")\n",
    "    alpha3 = tf.constant(0.05, dtype=np.float32, name=\"a3\")\n",
    "    in_u1 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"ull\")\n",
    "    in_v1 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"vll\")\n",
    "    in_u2 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"ulu\")\n",
    "    in_v2 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"vlu\")\n",
    "    in_u3 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"ulu\")\n",
    "    in_v3 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"ulu\")\n",
    "    labels_u1 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"lu1\")\n",
    "    labels_v1 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"lv1\")\n",
    "    labels_u2 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"lu2\")\n",
    "    weights_ll = tf.placeholder(tf.float32, [None, ], name=\"wll\")\n",
    "    weights_lu = tf.placeholder(tf.float32, [None, ], name=\"wlu\")\n",
    "    weights_uu = tf.placeholder(tf.float32, [None, ], name=\"wuu\")\n",
    "    cu1 = tf.placeholder(tf.float32, [None, ], name=\"CuLL\")\n",
    "    cv1 = tf.placeholder(tf.float32, [None, ], name=\"CvLL\")\n",
    "    cu2 = tf.placeholder(tf.float32, [None, ], name=\"CuLU\")\n",
    "\n",
    "    labels_zero_1 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"l0_1\")\n",
    "    labels_zero_2 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"l0_2\")\n",
    "    labels_zero_3 = tf.placeholder(tf.int32, [None, MAX_LEN], name=\"l0_3\")\n",
    "    \n",
    "    return [p_keep_conv, alpha1, alpha2, alpha3, in_u1, in_v1, in_u2,\\\n",
    "    in_v2, in_u3, in_v3, labels_u1, labels_v1,\\\n",
    "    labels_u2, weights_ll, weights_lu, weights_uu,\\\n",
    "    cu1, cv1, cu2, labels_zero_1, labels_zero_2, labels_zero_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# vanilla loss\n",
    "# cross entropy loss with logit and mask \n",
    "def vanilla_loss(logits, labels):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "    loss = tf.identity(loss)\n",
    "    loss *= tf.cast(tf.not_equal(labels, tf.zeros_like(labels)), loss.dtype)\n",
    "    loss = tf.reduce_sum(loss, 1)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# distance loss\n",
    "def distance_loss(scores_u, scores_v):\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=scores_u, labels=tf.nn.softmax(scores_v))\n",
    "    loss = tf.reduce_sum(loss, 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "num_gpus = len(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "    model = ByteNet(latent_dim=latent_dim, num_block_sets=num_block_sets)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    \n",
    "    with tf.variable_scope(tf.get_variable_scope()) as scope:\n",
    "        params = make_params()\n",
    "        with tf.device('/gpu:0'):\n",
    "\n",
    "            loss1, enc_u1, enc_v1, enc_u2 = tower_loss1(model, params)\n",
    "            scope.reuse_variables()\n",
    " \n",
    "        with tf.device('/gpu:1'):\n",
    "            \n",
    "            loss2 = tower_loss2(model, params, enc_u1, enc_v1, enc_u2)\n",
    "\n",
    "        #with tf.device('/gpu:2'):\n",
    "            loss_function = loss1 + loss2\n",
    "    \n",
    "\n",
    "    train = optimizer.minimize(loss_function)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Start running operations on the Graph. allow_soft_placement must be set to\n",
    "    # True to build towers on GPU, as some of the ops do not have GPU\n",
    "    # implementations.\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver(var_list=sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a minute I was here before?\n",
      "They made retreat into a forest.\n",
      "trees clothed in fresh leaves[with verdure]\n",
      "This is the reason for my argument.\n",
      "A long time ago, when there weren't any pencils or pens, people used bird feathers to write.\n",
      "Year and a half. I'm just saying.\n",
      "Remembering times in an empty park on a spring day.\n",
      "Is this supposed to indicate recalcitrant behaviour?\n",
      "I'm sorry you find the plot grotty.\n",
      "Log all requests and state changes.\n",
      "Circuit protected against transient low voltages\n",
      "My whole family came to my graduation.\n",
      "I didn't ask for her number, she gave it to me.\n",
      "See the element for parameters and defaults.\n",
      "An extra armoured brigade was asked for and not delivered.\n",
      "Notwithstanding those attacks we showed we can fight back.\n",
      "When fall comes, the leaves turn red and yellow.\n",
      "Yangsan Pusan National University Hospital(1st Prize)\n",
      "I think that that is the right approach.\n",
      "Searches the description property.\n",
      "No, he didn't, I have no idea who the guy is!\n",
      "By his desire to save people.\n",
      "Annals of the New York Academy of Sciences.\n",
      "User Selective Call Forwarding (Stage 1) (71KB)\n",
      "But for the rest of the break, we're going to Disneyland!\n",
      "No idea how he slipped by all the cameras.\n",
      "If sauce is too thick, add a little milk (or water).\n",
      "I don't ever want to feel this way again.\n",
      "Doesn't seem to be too serious.\n",
      "They don't even let us watch the programme.\n",
      "Dad, I like you so much.\n",
      "It is about three miles from here.\n",
      "======== EPOCH 1 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "1it [00:18, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195.621\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "2it [00:26, 15.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.811\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "3it [00:34, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190.774\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "4it [00:42, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190.137\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184.561\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "5it [00:55, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0: 4.65s, 599m] 9.46466498374939\n",
      "Wait a minute I was here before?\n",
      "They made retreat into a forest.\n",
      "trees clothed in fresh leaves[with verdure]\n",
      "This is the reason for my argument.\n",
      "A long time ago, when there weren't any pencils or pens, people used bird feathers to write.\n",
      "Year and a half. I'm just saying.\n",
      "Remembering times in an empty park on a spring day.\n",
      "Is this supposed to indicate recalcitrant behaviour?\n",
      "I'm sorry you find the plot grotty.\n",
      "Log all requests and state changes.\n",
      "Circuit protected against transient low voltages\n",
      "My whole family came to my graduation.\n",
      "I didn't ask for her number, she gave it to me.\n",
      "See the element for parameters and defaults.\n",
      "An extra armoured brigade was asked for and not delivered.\n",
      "Notwithstanding those attacks we showed we can fight back.\n",
      "When fall comes, the leaves turn red and yellow.\n",
      "Yangsan Pusan National University Hospital(1st Prize)\n",
      "I think that that is the right approach.\n",
      "Searches the description property.\n",
      "No, he didn't, I have no idea who the guy is!\n",
      "By his desire to save people.\n",
      "Annals of the New York Academy of Sciences.\n",
      "User Selective Call Forwarding (Stage 1) (71KB)\n",
      "But for the rest of the break, we're going to Disneyland!\n",
      "No idea how he slipped by all the cameras.\n",
      "If sauce is too thick, add a little milk (or water).\n",
      "I don't ever want to feel this way again.\n",
      "Doesn't seem to be too serious.\n",
      "They don't even let us watch the programme.\n",
      "Dad, I like you so much.\n",
      "It is about three miles from here.\n",
      "The won the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "I wast the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "The wou the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the tinn.\n",
      "The wathe the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "The wat the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "The wout the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "I wout the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "I was the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "The wor the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "I want the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "The wat the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "I we the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "I won the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the tin the te the\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "The wont the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "I won the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "I was the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "The was the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "I wout the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "I was the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "I won the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "I won the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "The wont the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "6it [01:02, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193.013\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "7it [01:10,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183.841\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "8it [01:25, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191.797\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-9973ffb3dd0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                                            \u001b[0mlabels_zero_1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ml0_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                            \u001b[0mlabels_zero_2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ml0_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                            labels_zero_3: l0_3})\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaywalnut310/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaywalnut310/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaywalnut310/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/jaywalnut310/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaywalnut310/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "[p_keep_conv, alpha1, alpha2, alpha3, in_u1, in_v1, in_u2,\\\n",
    "    in_v2, in_u3, in_v3, labels_u1, labels_v1,\\\n",
    "    labels_u2, weights_ll, weights_lu, weights_uu,\\\n",
    "    cu1, cv1, cu2, labels_zero_1, labels_zero_2, labels_zero_3] = params\n",
    "\n",
    "data_ko2en.print_index(data_ko2en.target[:batch_size]) \n",
    "for epoch in range(num_epochs):\n",
    "    print(\"======== EPOCH \" + str(epoch + 1) + \" ========\")\n",
    "\n",
    "    batches = batch_iter(batch_size=128)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    cnt = 0\n",
    "    for batch in tqdm(batches):\n",
    "\n",
    "        u1, v1, lu1, lv1, u3, v3, u2, v2, lu2, w_ll, w_lu, w_uu, c_ull, c_vll, c_ulu = batch\n",
    "        \n",
    "        l0_1 = np.zeros(u1.shape)\n",
    "        l0_2 = np.zeros(u2.shape)\n",
    "        l0_3 = np.zeros(u3.shape)\n",
    "        _, loss = sess.run([train, loss_function],\n",
    "                                feed_dict={in_u1: u1,\n",
    "                                           in_v1: v1,\n",
    "                                           in_u2: u2,\n",
    "                                           in_v2: v2,\n",
    "                                           in_u3: u3,\n",
    "                                           in_v3: v3,\n",
    "                                           labels_u1: lu1,\n",
    "                                           labels_v1: lv1,\n",
    "                                           labels_u2: lu2,\n",
    "                                           weights_ll: w_ll,\n",
    "                                           weights_lu: w_lu,\n",
    "                                           weights_uu: w_uu,\n",
    "                                           cu1: c_ull,\n",
    "                                           cv1: c_vll,\n",
    "                                           cu2: c_ulu,\n",
    "                                           p_keep_conv: 0.9,\n",
    "                                           labels_zero_1: l0_1,\n",
    "                                           labels_zero_2: l0_2,\n",
    "                                           labels_zero_3: l0_3})\n",
    "        epoch_loss += loss\n",
    "        print(loss, end='\\r')\n",
    "        cnt += 1\n",
    "        \n",
    "        if(cnt % 5 == 0):\n",
    "            print()\n",
    "            test_func()\n",
    "            saver.save(sess, './logs/models/ngm-model-%d.ckpt' % cnt, write_meta_graph=False)\n",
    "        \n",
    "    print()\n",
    "    print(\"Epoch_Loss\", epoch_loss/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Train data loaded.(total data=121597, total batch=3799)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import time\n",
    "from data import KOEN\n",
    "\n",
    "# set log level to debug\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "\n",
    "batch_size = 8  # batch size\n",
    "latent_dim = 100\n",
    "num_blocks = 2 # for encoder, decoder\n",
    "num_layers = 5 # 1, 2, 4, 8, 16\n",
    "\n",
    "##\n",
    "## TEST DATA SET\n",
    "##\n",
    "data_ko2en = KOEN(batch_size=batch_size, mode='test')\n",
    "\n",
    "input_dim = data_ko2en.voca_size\n",
    "input_max_len = data_ko2en.max_len\n",
    "\n",
    "graph = sess.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default(), tf.device('/gpu:1'):\n",
    "    x = tf.placeholder(tf.int32, [None, MAX_LEN], name='x')\n",
    "    y = tf.placeholder(tf.int32, [None, MAX_LEN], name='y')\n",
    "    p = tf.constant(1.0, dtype=tf.float32, name='p')\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=True) as scope:\n",
    "        enc_x = model.encoder(x)\n",
    "        logits_y = model.decoder(enc_x, y, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## SET WEIGHTS\n",
    "##\n",
    "train_vars = sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "var_names = [x.name for x in train_vars]\n",
    "\n",
    "weights = []\n",
    "for b in range(num_blocks):\n",
    "    weights.append([])\n",
    "    for i in range(num_layers):\n",
    "        weights[b].append([])\n",
    "        rate = 2**i\n",
    "        \n",
    "        w_names = 'decoder/res_block_%d_%d/w' % (b, rate)\n",
    "        w_names = [w_names+x+':0' for x in ['1', '2', '3']]\n",
    "        \n",
    "        for w_name in w_names:\n",
    "            weights[b][i].append(train_vars[var_names.index(w_name)])\n",
    "\n",
    "weights.append([])\n",
    "weights[-1].append(train_vars[var_names.index('output/w1:0')])\n",
    "weights[-1].append(train_vars[var_names.index('output/w2:0')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def simple_norm(x):\n",
    "    mean, var = tf.nn.moments(x, [1], keep_dims=True)\n",
    "    \n",
    "    return (x - mean) / tf.sqrt(var + 1e-5)\n",
    "\n",
    "\n",
    "def linear_block(w, inputs, state):\n",
    "    x = inputs\n",
    "    x = tf.matmul(state, w[0]) + tf.matmul(x, w[1])\n",
    "    x = simple_norm(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def linear_output(variables, inputs):\n",
    "    x = inputs\n",
    "    x = simple_norm(x)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    w0 = variables[0][0]\n",
    "    x = tf.matmul(x, w0)\n",
    "    x = simple_norm(x)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    w1 = variables[1][0]\n",
    "    x = tf.matmul(x, w1)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def linear_init(sess):\n",
    "    with sess.graph.as_default():\n",
    "        input_size = latent_dim * 2\n",
    "        inputs = tf.placeholder(tf.float32, [batch_size, input_size], name='inputs')\n",
    "        \n",
    "        h = inputs\n",
    "        # block op\n",
    "        init_ops = []\n",
    "        push_ops = []\n",
    "        pull_ops = []\n",
    "        qs = []\n",
    "        dummies = []\n",
    "        for b in range(num_blocks):\n",
    "            for i in range(num_layers):\n",
    "                rate = 2**i\n",
    "                \n",
    "                x = h\n",
    "                x = simple_norm(x)\n",
    "                x = tf.nn.relu(x)\n",
    "                x = tf.matmul(x, weights[b][i][0][0])\n",
    "                x = simple_norm(x)\n",
    "                x = tf.nn.relu(x)\n",
    "                \n",
    "                # fast op\n",
    "                state_size = latent_dim\n",
    "                    \n",
    "                q = tf.FIFOQueue(rate,\n",
    "                                 dtypes=tf.float32,\n",
    "                                 shapes=(batch_size, state_size))\n",
    "                dummy = tf.zeros((rate, batch_size, state_size))\n",
    "                init = q.enqueue_many(dummy)\n",
    "                pull_op = q.dequeue_many(rate)\n",
    "            \n",
    "                \n",
    "                state_ = q.dequeue()\n",
    "                push = q.enqueue([x])\n",
    "                init_ops.append(init)\n",
    "                push_ops.append(push)\n",
    "                pull_ops.append(pull_op)\n",
    "                qs.append(q)\n",
    "                dummies.append(dummy)\n",
    "                \n",
    "                # block op\n",
    "                x = linear_block(weights[b][i][1], x, state_)\n",
    "                x = tf.matmul(x, weights[b][i][2][0])\n",
    "\n",
    "                h = x + h\n",
    "\n",
    "        outputs = linear_output(weights[-1], h)\n",
    "\n",
    "        out_ops = [tf.nn.softmax(outputs)]\n",
    "        out_ops.extend(push_ops)\n",
    "    \n",
    "    return inputs, init_ops, out_ops, pull_ops, qs, dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run(sess, batch_src):\n",
    "    \n",
    "    predictions = []\n",
    "    batch = np.zeros(len(batch_src), 'int32')\n",
    "    x_enc = sess.run(enc_x, {x: batch_src})\n",
    "    \n",
    "    for step in range(input_max_len):\n",
    "        # make batch\n",
    "        batch = np.concatenate((x_enc[:,step], emb_y[batch]), axis=1)\n",
    "        feed_dict = {inputs: batch}\n",
    "        output = sess.run(out_ops, feed_dict=feed_dict)[0] # ignore push ops\n",
    "        \n",
    "        batch = np.argmax(output,1)\n",
    "        predictions.append(batch)\n",
    "\n",
    "    predictions_ = np.array(predictions).T\n",
    "    return predictions_\n",
    "\n",
    "def clean(sess):\n",
    "            \n",
    "    all_ops = []\n",
    "    all_ops.extend(pull_ops)\n",
    "    all_ops.extend(init_ops)\n",
    "    sess.run(all_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputs, init_ops, out_ops, pull_ops, qs, dummies = linear_init(sess)\n",
    "# Initialize queues.\n",
    "sess.run(init_ops)\n",
    "rates = sess.run([q.size() for q in qs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def copy_qs(sess, ids):\n",
    "    '''\n",
    "    Copy queue values of two elements in a batch\n",
    "    '''\n",
    "   \n",
    "    qs_vals = sess.run(pull_ops)\n",
    "    \n",
    "    dummies_dict = {}\n",
    "    for i in range(len(qs)):\n",
    "        q = qs[i]\n",
    "\n",
    "        q_val = qs_vals[i]\n",
    "        q_val = q_val[:, ids]\n",
    "        \n",
    "        dummies_dict[dummies[i]] = q_val\n",
    "\n",
    "    sess.run(init_ops, dummies_dict)\n",
    "    \n",
    "def beam_run(sess, batch_src, batch_size=4, beam_size=8):\n",
    "    # Beam Search Variables\n",
    "    end_flags = np.zeros([batch_size*beam_size],dtype='int32')\n",
    "    storage_c = np.zeros([batch_size*beam_size, input_max_len], dtype='int32')\n",
    "    storage_p = np.zeros([batch_size*beam_size, input_max_len], dtype='float32')\n",
    "    loglike = np.zeros([batch_size*beam_size], dtype='float32')\n",
    "    \n",
    "    predictions = []\n",
    "    batch = np.zeros(batch_size*beam_size, 'int32')\n",
    "    x_enc = sess.run(enc, {x: batch_src})\n",
    "    x_enc = x_enc[np.array([[i] * beam_size for i in range(len(x_enc))]).flatten()] # upsampling\n",
    "    \n",
    "    for step in range(input_max_len):\n",
    "        # make batch\n",
    "        batch = np.concatenate((x_enc[:,step], emb_y[batch]), axis=1)\n",
    "        feed_dict = {inputs: batch}\n",
    "        \n",
    "        output = sess.run(out_ops, feed_dict=feed_dict)[0] # ignore push ops\n",
    "        \n",
    "        # Beam search\n",
    "        M_p = output # M_p : unnormalized probabilities. [batch_size*beam_size, input_dim]\n",
    "        M_p = M_p / np.sum(M_p, axis=1).reshape(-1,1) # normalize probabilities\n",
    "        \n",
    "        # block ended seqs\n",
    "        block_ids = np.where(end_flags)[0]\n",
    "\n",
    "        M_p[M_p<1e-45] = 1e-45\n",
    "        # calculate log_likelihoods\n",
    "        M_l = loglike.reshape(-1,1) + (1-end_flags).reshape(-1,1)*np.log(M_p)\n",
    "        \n",
    "        if step==0: # remove all except first values for similarity breaking\n",
    "            mask = np.array([[1] + [0]*(beam_size-1)]*batch_size).flatten()\n",
    "            M_l = M_l * mask.reshape(-1,1)\n",
    "            M_l += np.array([0, -np.inf])[(1-mask)].reshape(-1,1)\n",
    "\n",
    "        # calculate scores\n",
    "        len_y = np.argmin(np.not_equal(storage_c,0), axis=1) + 1\n",
    "        len_penalty = (((1 + len_y) / (1 + 5)) ** 0.65)\n",
    "        M_s = M_l / len_penalty.reshape(-1,1)\n",
    "\n",
    "        # find indices of top-n scores\n",
    "        M_s_flatten = M_s.reshape(-1, input_dim*beam_size)\n",
    "        col_ids = np.flip(np.argsort(M_s_flatten, axis=1), axis=1)[:,:beam_size]\n",
    "        col_ids = col_ids.reshape(-1)\n",
    "        row_ids = np.array([[i*beam_size]*beam_size for i in range(batch_size)]).flatten()\n",
    "        row_ids += col_ids // input_dim\n",
    "        col_ids = col_ids % input_dim\n",
    "\n",
    "        # update variables\n",
    "        eos = 1\n",
    "\n",
    "        end_flags = end_flags[row_ids]\n",
    "\n",
    "        storage_c = storage_c[row_ids]\n",
    "        storage_c[:,step] = col_ids * (1-end_flags)\n",
    "\n",
    "        storage_p = storage_p[row_ids]\n",
    "        storage_p[:,step] = M_p[row_ids, col_ids] * (1-end_flags)\n",
    "\n",
    "        loglike = M_l[row_ids, col_ids] * (1-end_flags) + loglike * end_flags \n",
    "\n",
    "        end_flags = (col_ids==eos) * (1-end_flags) + end_flags\n",
    "        \n",
    "        batch = storage_c[:,step]\n",
    "        if((batch==0).all()):\n",
    "            break\n",
    "        \n",
    "        copy_qs(sess, row_ids)\n",
    "        \n",
    "    predictions_ = storage_c[np.arange(batch_size) * beam_size]\n",
    "    return predictions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_func():\n",
    "    global emb_y\n",
    "    \n",
    "    t_avg = 10.\n",
    "    ret = []\n",
    "    emb_y = sess.run(train_vars[var_names.index('input/emb_y:0')])\n",
    "    for i in range(1): #data_ko2en.num_batch * beam_size):\n",
    "        t_str = time.time()\n",
    "\n",
    "        predictions = run(sess, data_ko2en.source[i*batch_size:(i+1)*batch_size])\n",
    "        clean(sess)\n",
    "        ret.extend(data_ko2en.print_index(predictions, sysout=False))\n",
    "\n",
    "        t_elp = time.time() - t_str\n",
    "        t_avg = 0.9*t_avg + 0.1*t_elp\n",
    "        t_rem = t_avg * (data_ko2en.num_batch-i-1)\n",
    "        print('[%d: %.2fs, %dm]' % (i, t_elp, t_rem//60), t_avg)\n",
    "    \n",
    "    data_ko2en.print_index(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
