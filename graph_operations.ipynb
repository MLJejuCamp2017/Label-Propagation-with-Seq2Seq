{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "# similarity analysis using GPUs\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load all data (vectors)\n",
    "L = sp.sparse.load_npz('./data/graph/labeled.npz')\n",
    "U = sp.sparse.load_npz('./data/graph/unlabeled.npz')\n",
    "M = sp.sparse.vstack([L,U]) # combining labeled data with unlabeled data\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "# and change type from 'float64' to 'float32' since 'faiss' doesn't support 'float64' type\n",
    "M = M.toarray()\n",
    "M = M.astype('float32')\n",
    "M = normalize(M) # L2 Norm before calculating cosine similarity\n",
    "\n",
    "last_index_l = L.shape[0]\n",
    "last_index_u = last_index_l + U.shape[0]\n",
    "\n",
    "# we only keep the closest neighbors\n",
    "max_neighs = 5\n",
    "size = M.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\" FAISS operations \"\"\"\n",
    "res = faiss.StandardGpuResources()\n",
    "index = faiss.GpuIndexFlatIP(res, M.shape[1]) # build the index\n",
    "\n",
    "index.add(M) # add vectors to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 978/978 [02:23<00:00,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "batch_num = int(np.ceil(size / batch_size))\n",
    "\n",
    "sims, inds = [], []\n",
    "\n",
    "for i in tqdm(range(batch_num)):\n",
    "    # actual search\n",
    "    similarities, indices = index.search(M[i*batch_size:int(np.min([(i+1)*batch_size, size]))],max_neighs+1)\n",
    "    \n",
    "    # remove self-references\n",
    "    batch_ids = np.vstack(np.arange(i*batch_size, int(np.min([(i+1)*batch_size, size]))))\n",
    "    xs, ys = np.where(indices!=batch_ids)\n",
    "    similarities[xs,ys] = 0\n",
    "    \n",
    "    sims.extend(similarities)\n",
    "    inds.extend(indices)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = dict()\n",
    "edges_weights = dict()\n",
    "edges_ll = list()\n",
    "edges_lu = list()\n",
    "edges_uu = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 977936/977936 [00:21<00:00, 46284.86it/s] \n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(size)):\n",
    "    neighbors_indices = list(inds[i][sims[i].argsort()[-max_neighs::][::-1]])\n",
    "    correct_indices = [j for j in neighbors_indices if i < j]\n",
    "    graph.update({i:correct_indices})\n",
    "\n",
    "    n = len(correct_indices)\n",
    "\n",
    "    if n > 0:\n",
    "        edges = list(zip([i] * n, correct_indices))\n",
    "        take_indices = [np.where(inds[i]==x)[0][0] for x in correct_indices]\n",
    "        edges_weights.update(dict(zip(edges,np.take(sims[i],take_indices))))\n",
    "\n",
    "        for j in correct_indices:\n",
    "            if (0 <= i < last_index_l) and (0 <= j < last_index_l):\n",
    "                edges_ll.append((i,j))\n",
    "            elif (0 <= i < last_index_l) and (last_index_l <= j < last_index_u):\n",
    "                edges_lu.append((i,j))\n",
    "            else:\n",
    "                edges_uu.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611285 1221149 609450 2441884\n"
     ]
    }
   ],
   "source": [
    "print(len(edges_ll), len(edges_lu), len(edges_uu), len(edges_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# save to file the data structure that we worked so hard to compute\n",
    "pickle.dump(dict(graph), open(\"./data/graph/graph.p\", \"wb\"))\n",
    "pickle.dump(dict(edges_weights), open(\"./data/graph/edges_weights.p\", \"wb\"))\n",
    "pickle.dump(list(edges_ll), open(\"./data/graph/edges_ll.p\", \"wb\"))\n",
    "pickle.dump(list(edges_lu), open(\"./data/graph/edges_lu.p\", \"wb\"))\n",
    "pickle.dump(list(edges_uu), open(\"./data/graph/edges_uu.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingsGraph:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.graph = nx.Graph()\n",
    "        #self.graph = pickle.load(open(\"./data/graph/graph.p\", \"rb\"))\n",
    "        edges_ll = pickle.load(open(\"./data/graph/edges_ll.p\", \"rb\"))\n",
    "        edges_lu = pickle.load(open(\"./data/graph/edges_lu.p\", \"rb\"))\n",
    "        edges_uu = pickle.load(open(\"./data/graph/edges_uu.p\", \"rb\"))\n",
    "        self.edges = edges_ll + edges_lu + edges_uu\n",
    "        self.edges_weights = pickle.load(open(\"./data/graph/edges_weights.p\", \"rb\"))\n",
    "\n",
    "        for (u,v) in self.edges:\n",
    "            self.graph.add_edge(u, v, weight=self.edges_weights.get((u, v)))\n",
    "\n",
    "    def weight(self,u,v):\n",
    "        if u < v:\n",
    "            return self.edges_weights.get((u,v))\n",
    "        else:\n",
    "            return self.edges_weights.get((v,u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Remove Nodes with Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from embeddings_graph import EmbeddingsGraph\n",
    "from bytenet import ByteNet\n",
    "from data_koen import KOEN\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Train data loaded.(total data=487558, total batch=15236)\n",
      "INFO:tensorflow:Train data loaded.(total data=487527, total batch=15235)\n"
     ]
    }
   ],
   "source": [
    "graph = EmbeddingsGraph().graph\n",
    "batch_size = 32\n",
    "\n",
    "data = KOEN(batch_size, 'train')\n",
    "data2 = KOEN(batch_size, 'train.mono')\n",
    "\n",
    "with open('./data/raw/ko.train', 'r') as f:\n",
    "    ss_L = f.readlines()\n",
    "    ss_L = [unicodedata.normalize(\"NFKD\", unicode_str[:-1]) for unicode_str in ss_L]\n",
    "    \n",
    "with open('./data/raw/ko.train.mono', 'r') as f:\n",
    "    ss_U = f.readlines()\n",
    "    ss_U = [unicodedata.normalize(\"NFKD\", unicode_str[:-1]) for unicode_str in ss_U]\n",
    "\n",
    "l = len(ss_L) #last index of labeled samples\n",
    "u = l + len(ss_U) #last index of all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.voca_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.voca_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for key, value in data2.ids.items():\n",
    "    data.ids[key+l] = data.num_data + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.source.extend(data2.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "##### remove #####\n",
    "##################\n",
    "for (u, v) in graph.edges():\n",
    "    try:\n",
    "        data.ids[u]\n",
    "        data.ids[v]\n",
    "    except:\n",
    "        graph.remove_edge(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.source = np.array(data.source)\n",
    "data.target = np.array(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def label(i):\n",
    "    if 0 <= i < l:\n",
    "        return data.target[data.ids[i]]\n",
    "\n",
    "\n",
    "def next_batch(h_edges, start, finish):\n",
    "    \"\"\"\n",
    "    Helper function for the iterator, note that the neural graph machines,\n",
    "    due to its unique loss function, requires carefully crafted inputs\n",
    "\n",
    "    Refer to the Neural Graph Machines paper, section 3 and 3.3 for more details\n",
    "    \"\"\"\n",
    "    edges_ll = list()\n",
    "    edges_lu = list()\n",
    "    edges_uu = list()\n",
    "    weights_ll = list()\n",
    "    weights_lu = list()\n",
    "    weights_uu = list()\n",
    "    batch_edges = h_edges[start:finish]\n",
    "    batch_edges = np.asarray(batch_edges)\n",
    "\n",
    "    for i, j in batch_edges[:]:\n",
    "        if (0 <= i < l) and (0 <= j < l):\n",
    "            edges_ll.append((i, j))\n",
    "            weights_ll.append(graph.get_edge_data(i,j)['weight'])\n",
    "        elif (0 <= i < l) and (l <= j < u):\n",
    "            edges_lu.append((i, j))\n",
    "            weights_lu.append(graph.get_edge_data(i,j)['weight'])\n",
    "        else:\n",
    "            edges_uu.append((i, j))\n",
    "            weights_uu.append(graph.get_edge_data(i,j)['weight'])\n",
    "\n",
    "    u_ll = [e[0] for e in edges_ll]\n",
    "\n",
    "    # number of incident edges for nodes u\n",
    "    c_ull = [1 / len(graph.edges(n)) for n in u_ll]\n",
    "    v_ll = [e[1] for e in edges_ll]\n",
    "    c_vll = [1 / len(graph.edges(n)) for n in v_ll]\n",
    "    nodes_ll_u = data.source[[data.ids[x] for x in u_ll]]\n",
    "\n",
    "    labels_ll_u = np.vstack([label(n) for n in u_ll])\n",
    "\n",
    "    nodes_ll_v = data.source[[data.ids[x] for x in v_ll]]\n",
    "\n",
    "    labels_ll_v = np.vstack([label(n) for n in v_ll])\n",
    "\n",
    "    u_lu = [e[0] for e in edges_lu]\n",
    "    c_ulu = [1 / len(graph.edges(n)) for n in u_lu]\n",
    "    nodes_lu_u = data.source[[data.ids[x] for x in u_lu]]\n",
    "    nodes_lu_v = data.source[[data.ids[x] for x in [e[1] for e in edges_lu]]]\n",
    "\n",
    "    labels_lu = np.vstack([label(n) for n in u_lu])\n",
    "\n",
    "    nodes_uu_u = data.source[[data.ids[x] for x in [e[0] for e in edges_uu]]]\n",
    "    nodes_uu_v = data.source[[data.ids[x] for x in [e[1] for e in edges_uu]]]\n",
    "\n",
    "    return nodes_ll_u, nodes_ll_v, labels_ll_u, labels_ll_v, \\\n",
    "           nodes_uu_u, nodes_uu_v, nodes_lu_u, nodes_lu_v, \\\n",
    "           labels_lu, weights_ll, weights_lu, weights_uu, \\\n",
    "           c_ull, c_vll, c_ulu\n",
    "\n",
    "\n",
    "def batch_iter(batch_size):\n",
    "    \"\"\"\n",
    "        Generates a batch iterator for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    data_size = len(graph.edges())\n",
    "\n",
    "    edges = np.random.permutation(graph.edges())\n",
    "\n",
    "    num_batches = int(data_size / batch_size)\n",
    "\n",
    "    if data_size % batch_size > 0:\n",
    "        num_batches = int(data_size / batch_size) + 1\n",
    "\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        yield next_batch(edges,start_index,end_index)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf_r1.0_3.5]",
   "language": "python",
   "name": "conda-env-tf_r1.0_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
